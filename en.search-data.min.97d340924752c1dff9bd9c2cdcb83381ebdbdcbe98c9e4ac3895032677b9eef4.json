[{"id":0,"href":"/telemetry/01-exporting-eventsource-locally/","title":"1. Exporting EventSource logs to CSV","section":"Telemetry","content":" 1. Exporting EventSource logs to CSV # Here\u0026rsquo;s a scenario - you\u0026rsquo;re building a service and you\u0026rsquo;re emitting logs as you should be. But your service is configured to send those logs to a log collector. You don\u0026rsquo;t have a lot of budget and you\u0026rsquo;re using a log collector in production but locally you only write out to console (or Debug window in Visual Studio). Maybe you want to persist your logs over a few sessions, maybe you have an automated test environment which hosts your service without console output being saved anywhere. Sure you might implement a file sink for your logs.\nOr you might decide to capture the data from the EventSource your service might be already using.\nEventSource? # This post is for .NET devs. What is an EventSource? It\u0026rsquo;s an old API designed in .NET Framework to pipe events into the ETW (Event Tracing for Windows) system. It has been revamped in .NET Core to be multi-platform and it\u0026rsquo;s the provider of data into dotnet-trace tool. Many internal components emit EventSource events (like GC for example).\nAnd some logging libraries do as well. For example Microsoft.Extensions.Logging is able to export logs to an event source. See AddEventSourceLogger method. It\u0026rsquo;s on by default with the default Host builder.\nReferences:\nEventSource - Getting started EventSource provider PerfView library # So you can capture events using dotnet-trace and view them in standalone PerfView application. It\u0026rsquo;s powerful but not the most comfortable way of dealing with text logs. Not that CSV is much better, but maybe a little.\nPerfView ships either as a GUI app or as a NuGet package Microsoft.Diagnostics.Tracing.TraceEvent. You can use the library to either:\nopen a file produced by dotnet-trace - using new EventPipeEventSource(inputFileName). listen to any events from a provider - using new TraceEventSession(sessionName) with session.EnableProvider(providerName). I opted for the second one, because it allows us to start capturing events as soon as the application starts and it can simultaneously process events from multiple processes.\nFor example to listen to events from the Microsoft-Extensions-Logging EventSource ( source):\nusing (TraceEventSession session = new TraceEventSession(\u0026#34;MyTraceSession\u0026#34;)) { // Enable the ETW session to listen for events from an EventSource session.EnableProvider(\u0026#34;Microsoft-Extensions-Logging\u0026#34;); // filter events to only a specific type and process with callback // we might only listen for MessageJson events for example session.Source.Dynamic.AddCallbackForProviderEvents( (string providerName, string eventName) =\u0026gt; providerName == \u0026#34;Microsoft-Extensions-Logging\u0026#34; \u0026amp;\u0026amp; eventName == \u0026#34;MessageJson\u0026#34; ? EventFilterResponse.AcceptEvent : EventFilterResponse.RejectEvent;, (TraceEvent data) =\u0026gt; {/* do stuff with the event */}); session.Source.Process(); // Listen to events and invoke callback for events in the source } The AddCallbackForProviderEvents takes two functions:\nevent filter - which allows you to specify which events your processor supports, event callback - which allows you to process events one by one. So for me the callback read data from TraceEvent object and emitted them to CSV files. The event object contains info about which process emitted it and the payload with data.\nYou can either process event with typed handlers (which I didn\u0026rsquo;t manage to do because you need to generate a C# class from ETW manifest) or a dynamic handler. In this case I\u0026rsquo;m reading dynamic properties from payload like this:\nTimeStamp = data.TimeStamp.ToString(\u0026#34;s\u0026#34;), ThreadId = data.ThreadID.ToString(\u0026#34;X\u0026#34;), TagId = new EventId((int)data.PayloadByName(\u0026#34;EventId\u0026#34;)).ToTagId(), Level = ((LogLevel)data.PayloadByName(\u0026#34;Level\u0026#34;)).ToString(), LoggerName = data.PayloadStringByName(\u0026#34;LoggerName\u0026#34;), Message = data.PayloadStringByName(\u0026#34;FormattedMessage\u0026#34;), ExceptionDetails = data.PayloadStringByName(\u0026#34;ExceptionJson\u0026#34;), If your event source emits structures, you can do this (IDictionary\u0026lt;string, object\u0026gt;)data.PayloadByName(\u0026quot;ComplexColumn\u0026quot;).\nSo I wrote a little bit more code to use the CsvHelper library and write out the CSV files. I\u0026rsquo;m not gonna post all of the code but it goes a little bit like this:\nOn each event:\nGet CSV file name (process name + PID), Create (or get from cache) an instance of the CsvWriter for the file name: if the file didn\u0026rsquo;t exist we create it and write the header, otherwise just append to it Extract a row of data from event, Write a row to the CSV file. References:\nPerfView application tutorials The Microsoft.Diagnostics.Tracing.TraceEvent Library The TraceEvent Library Programmers Guide dotnet-trace tool CsvHelper website Ways to query a CSV file with SQL Building my tool # I decided to build my tool (which was like 4 C# files) into a single standalone executable, because I needed to deploy it into our test environment to collect logs from automated testing.\nAn alternative is to create your tool in a portable way as a dotnet tool but to use it the machine needs to have .NET SDK installed.\nSo here\u0026rsquo;s my configuration to publish a single file:\n\u0026lt;DebugType\u0026gt;Embedded\u0026lt;/DebugType\u0026gt; \u0026lt;SelfContained\u0026gt;true\u0026lt;/SelfContained\u0026gt; \u0026lt;PublishSingleFile\u0026gt;true\u0026lt;/PublishSingleFile\u0026gt; \u0026lt;GenerateDocumentationFile\u0026gt;false\u0026lt;/GenerateDocumentationFile\u0026gt; \u0026lt;EnableCompressionInSingleFile\u0026gt;true\u0026lt;/EnableCompressionInSingleFile\u0026gt; \u0026lt;IncludeAllContentForSelfExtract\u0026gt;true\u0026lt;/IncludeAllContentForSelfExtract\u0026gt; I was able to call dotnet publish and get a nice executable.\nBut I wanted to package it into a NuGet package which would contain just a tools folder with the executable. This took me two days to figure out fully.\nFirst, we want to enabling packing:\n\u0026lt;IsPackable\u0026gt;true\u0026lt;/IsPackable\u0026gt; Next, since we don\u0026rsquo;t want to include build outputs, but instead publish output, we will disable that:\n\u0026lt;!-- don\u0026#39;t include DLL files produced by this project because we\u0026#39;re manually adding publish output --\u0026gt; \u0026lt;IncludeBuildOutput\u0026gt;false\u0026lt;/IncludeBuildOutput\u0026gt; Now, I\u0026rsquo;m going to add two targets - first one triggers the publish action after building and before packing, second one includes the publish output in the package:\n\u0026lt;Target Name=\u0026#34;PublishBeforePack\u0026#34; BeforeTargets=\u0026#34;GenerateNuspec\u0026#34; AfterTargets=\u0026#34;Build\u0026#34;\u0026gt; \u0026lt;CallTarget Targets=\u0026#34;Publish\u0026#34; /\u0026gt; \u0026lt;/Target\u0026gt; \u0026lt;Target Name=\u0026#34;UpdatePackOutput\u0026#34; BeforeTargets=\u0026#34;GenerateNuspec\u0026#34; AfterTargets=\u0026#34;Publish\u0026#34;\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;_PackageFiles Include=\u0026#34;@(PublishItemsOutputGroupOutputs)\u0026#34;\u0026gt; \u0026lt;FinalOutputPath\u0026gt;%(PublishItemsOutputGroupOutputs.OutputPath)\u0026lt;/FinalOutputPath\u0026gt; \u0026lt;PackagePath\u0026gt;tools\u0026lt;/PackagePath\u0026gt; \u0026lt;/_PackageFiles\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Target\u0026gt; Finally, since the package has just a tool, it doesn\u0026rsquo;t need any dependencies. We can read online that property PrivateAssets=\u0026quot;all\u0026quot; removes the dependency from nuspec. But it also removes it from publish. After a while I found some issue on GitHub that mentioned there\u0026rsquo;s another property called Publish and setting it to true preserves the desired behavior for publishing.\n\u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;CsvHelper\u0026#34; Publish=\u0026#34;True\u0026#34; PrivateAssets=\u0026#34;all\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;Microsoft.Diagnostics.Tracing.TraceEvent\u0026#34; Publish=\u0026#34;True\u0026#34; PrivateAssets=\u0026#34;all\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; But now that we\u0026rsquo;ve gotten rid of library dependencies we need to do remove the framework dependency as well:\n\u0026lt;!-- suppress framework dependency when there\u0026#39;s no library dependencies due to PrivateAssets=all --\u0026gt; \u0026lt;SuppressDependenciesWhenPacking\u0026gt;true\u0026lt;/SuppressDependenciesWhenPacking\u0026gt; That\u0026rsquo;s it. Well, there\u0026rsquo;s one thing you need to do if you\u0026rsquo;re still getting errors - disable IncludeSymbols property which would generate the snupkg file.\nWith the package published to a NuGet source, you can use the following to extract the tool to a projects output:\n\u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;LocalLogsExport\u0026#34; Version=\u0026#34;1.0.0\u0026#34; ExcludeAssets=\u0026#34;all\u0026#34; GeneratePathProperty=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;None Include=\u0026#34;$(PkgLocalLogsExport)\\tools\\*.exe\u0026#34; CopyToOutputDirectory=\u0026#34;PreserveNewest\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; References:\nCreate dotnet tool Publish single file MSBuild debugging tool this is a life saver MSBuild targets documentation CallTarget task documentation PackageReference documentation Nuspec documentation I was inspecting the nuspec generate in the obj folder Suppress the \u0026lt;dependencies\u0026gt; element when packing a project #5132 NU5017 reported for symbol packages without clarifying this #10372 that was the issue bugging me a long time Graceful stopping # The way I wrote the tool the CSV file wasn\u0026rsquo;t being flushed on each event. So if you killed it, you\u0026rsquo;d loose some rows only collected in memory. Instead I subscribed to the console interrupt event to try to enable disposal of the CSV writer.\nprivate static void ConfigureProcesTermination(TraceEventSession session) { Console.CancelKeyPress += (_, args) =\u0026gt; { args.Cancel = true; // prevent process from being terminated before cleanup session.Source.StopProcessing(); // allow call to session.Source.Process() to return // and nicely exit the using block }; // if we are being stopped more forcefully let\u0026#39;s try to at least clean up the session from the system AppDomain.CurrentDomain.ProcessExit += (_, _) =\u0026gt; session.Stop(noThrow: true); } And then I was automating execution of my tool with Powershell and had to write this monstrosity to gracefully stop the process:\n# Stop trace capture gracefully (so that any buffers are flushed to disk) by sending it a CTRL+C signal # Apparently there\u0026#39;s no native way to do it in Powershell, so we\u0026#39;re calling a kernel function # see https://stackoverflow.com/a/64930077 # The FreeConsole and AttachConsole isolates the process from the current Powershell process (otherwise the Ctrl+C kills the script itself as well) $MemberDefinition = \u0026#39; [DllImport(\u0026#34;kernel32.dll\u0026#34;)]public static extern bool FreeConsole(); [DllImport(\u0026#34;kernel32.dll\u0026#34;)]public static extern bool AttachConsole(uint p); [DllImport(\u0026#34;kernel32.dll\u0026#34;)]public static extern bool GenerateConsoleCtrlEvent(uint e, uint p); public static void SendCtrlC(uint p) { FreeConsole(); if (AttachConsole(p)) { GenerateConsoleCtrlEvent(0, p); FreeConsole(); } AttachConsole(uint.MaxValue); }\u0026#39; Add-Type -Name \u0026#39;Console\u0026#39; -Namespace \u0026#39;Process\u0026#39; -MemberDefinition $MemberDefinition $(Get-Process -Name \u0026#39;LocalLogsExport\u0026#39;).Id | foreach { [Process.Console]::SendCtrlC($_) } Wait-Process -Name \u0026#39;LocalLogsExport\u0026#39; -Timeout 30 -ErrorAction Ignore "},{"id":1,"href":"/web/01-isolated-features/","title":"1. Isolated web features within a single ASP.NET Core service","section":"Web","content":" 1. Isolated web features within a single ASP.NET Core service # We\u0026rsquo;ve been discussion recently at work how we can increase our velocity for the team. I went to think and saw that a lot of new projects have a high upfront cost when it comes to provisioning the resources, setting up the build and deployment pipelines, etc. This is fine for longer running projects but would be delaying quick experiments much more than it should.\nI came up with the question - is it possible to host multiple separate projects within a single ASP.NET Core service? This would make us go once through the initial setup, allow the team to create multiple experiments and when an experiment is deemed good for running long term it would get extracted into a new fully fledged service so that it can be scaled up.\nThe key thing I had to worry about here is making the features as independent from the host and each other as possible, while keeping the cost of adding a new experiment low. Interdependence could cause a lot of trouble during extraction.\nI tried to see if there was anything already done for this topic under such terms as \u0026ldquo;multi-tenant\u0026rdquo;, but the articles are generally focused on multiple tenants using the same app, vs hosting multiple tenant apps in the same service.\nRequest pipeline and branching # ASP.NET Core allows you to configure branches in your request pipeline ( docs). Each branch can be running different middlewares, possibly configured differently. This is great because it means we can isolate the experiments - instead of them relying on the shared middleware setup, they need to explicitly write up their own request pipeline which can later be easily transferred during extraction.\nThe branching can either preserve tha path in the request or move the prefix by which we branched to the path base. Routing is applied on top of a path base. With manually mapped endpoints (e.g. using MapGet()) this can be used easily to isolate the path prefix from the paths in routes. However, my use case is about making it work with controllers. When you say MapControllers() it applies to all controllers known to the application. And unfortunately controllers are resolved at the host level, not at the request pipeline level. This means that if two features use the / route we can\u0026rsquo;t really differentiate that across branches. Hence for me I had to set preserveMatchedPathSegment: true and add a prefix to each route in a given feature.\n// feature initializer return appBuilder.Map(routePrefix, preserveMatchedPathSegment: true, app =\u0026gt; { // configure feature specific pipeline }); Single startup # Something I had to discover - when using ASP.NET Core we don\u0026rsquo;t get the same treatment of building request pipelines as we do with configuring services. Likely because order of adding distinct services doesn\u0026rsquo;t matter, but order of adding middleware does. Therefore all middleware must be configured within a single Startup class.\nIn my case I wanted each feature to be independent and have it\u0026rsquo;s own method for configuring the pipeline. I did a quick trick of creating a wrapper class that I can put in DI and then resolve in my Startup to apply feature specific services.\ninternal class IsolatedFeatureInitializer { public Action\u0026lt;IApplicationBuilder\u0026gt; Initializer { get; } public IsolatedFeatureInitializer(Action\u0026lt;IApplicationBuilder\u0026gt; initializer) { Initializer = initializer; } } // Startup public void Configure(IApplicationBuilder app) { // any middleware before will be executed for all requests IEnumerable\u0026lt;IsolatedFeatureInitializer\u0026gt; initializers = app.ApplicationServices.GetServices\u0026lt;IsolatedFeatureInitializer\u0026gt;(); foreach (IsolatedFeatureInitializer initializer in initializers) { initializer.Initializer(app); } // any middleware after will be executed only for non-isolated requests } Discovering controllers from other assemblies # My extension methods for feature management were in a project that didn\u0026rsquo;t reference the features. As such the controllers were not auto-discovered and I had to add:\nservices.AddMvcCore().AddApplicationPart(typeof(Feature).Assembly); Reference: When ASP.NET Core can\u0026rsquo;t find your controller: debugging application parts.\nAutomatic route prefix for controllers and Swagger groups # I was able to use an MVC convention to modify all controllers in the feature assembly.\nservices.AddMvcCore(c =\u0026gt; { c.Conventions.Add(new IsolatedFeatureConvention(routePrefix, typeof(Feature).Assembly)); }) internal class IsolatedFeatureConvention : IActionModelConvention { private readonly string m_featureName; private readonly Assembly m_sourceAssembly; public IsolatedFeatureConvention(string featureName, Assembly sourceAssembly) { m_featureName = featureName; m_sourceAssembly = sourceAssembly; } public void Apply(ActionModel action) { if (action.Controller.ControllerType.Assembly == m_sourceAssembly) { // apply group name to separate the actions for Swagger display // cannot start with / as it is removed from the swagger document url action.ApiExplorer.GroupName = m_featureName.TrimStart(\u0026#39;/\u0026#39;); // enforce a route prefix for the feature actions AttributeRouteModel routePrefix = new(new RouteAttribute(m_featureName)); foreach (SelectorModel selector in action.Selectors) { if (selector.AttributeRouteModel != null) { // in order for this to work the controller cation cannot have a route prefix starting with \u0026#39;/\u0026#39; which is considered an override selector.AttributeRouteModel = AttributeRouteModel.CombineAttributeRouteModel(routePrefix, selector.AttributeRouteModel); } else { selector.AttributeRouteModel = routePrefix; } } } } } Further reading: GroupName -\u0026gt; Swagger doc, Swagger UI, if you want to include version: this and this.\nIsolated DI # Ideally the features would be completely separated with regards to dependencies. There are some things that need to be defined in the host (e.g. hosted services, healthchecks, controller configuration), but most dependencies will be needed in the context of requests. Luckily for us, the dependencies are resolved in layers. A middleware is created using dependencies resolved as the pipeline progresses to that middleware. Same for controllers. It means we can override the value of HttpContext.RequestServices with another service provider and any middleware further down in the pipeline will resolve its dependencies using the override.\ninternal class IsolatedFeatureMiddleware { private readonly RequestDelegate m_next; private readonly IServiceProvider m_featureServices; public IsolatedFeatureMiddleware(RequestDelegate next, IServiceProvider featureServices) { m_next = next; m_featureServices = featureServices; } public async Task InvokeAsync(HttpContext context) { IServiceProvider originalProvider = context.RequestServices; try { using IServiceScope scope = m_featureServices.CreateScope(); context.RequestServices = scope.ServiceProvider; await m_next(context); } finally { context.RequestServices = originalProvider; } } } You may want to create links back to the host for singleton instances. The way I went about it is that I clone the IServiceCollection used to configure the host, while redirecting singleton references to the hosts IServiceProvider (except for open generic types).\n"},{"id":2,"href":"/homelab/01-mini-server-setup/","title":"1. Mini server setup","section":"Homelab","content":" 1. Mini server setup # The idea # I was browsing reddit, as one does, and stumbled over the r/MiniPC and found a spreadsheet with a ton of models of small computers. I already was thinking of having a small server at home and decided to find some nice small computer that wouldn\u0026rsquo;t be very expensive. Finally I managed to find Beelink EQR5 on Amazon with a discount of like 30%. I decided to get it and in turn stop paying for a cheap VPS (8 euro a month). I get more power, more ram, more storage.\nWhat would I use it for? For one, I need a server for hosting my dev projects by the rule of having a deployment as part of the development lifecycle. And then I took inspiration from r/SelfHosted and decided I want to run NextCloud so that I can store my photos (especially of my son) without storage limitations.\nStep 1: Operating system # I chose Debian LTS. I\u0026rsquo;m familiar with the way Debian family systems are working and want something reliable. I installed it without GUI, just with SSH server.\nWhile I used WiFi during installation, I wanted to use ethernet later but it was disabled (check with lshw -c network -sanitize). I could temporarily turn it on with ip link set \u0026lt;name\u0026gt; up and to enable it permanently I edited /etc/network/interfaces to add auto \u0026lt;name\u0026gt; and iface \u0026lt;name\u0026gt; inet dhcp lines.\nStep 2: Extra storage # The Beelink came with 500GB SSD and 16GB ram. I have a small 2TB hard drive over USB3.0 which I decided to plug in there for storing my files.\nDecided to try and configure autofs to mount that hard drive automatically. I had used fstab in the past and had difficulties when the drive was not available at boot.\n# /etc/auto.master # Cutom mount point under /mnt directory /mnt /etc/auto.mnt --timeout=60 # /etc/auto.mnt # This will mount /mnt/media # in async mode (faster but data could be lost if server crashed) # preventing device file nodes and preventing setuid (executing file as its owner) media -fstype=auto,async,nodev,nosuid :/dev/sda1 Step 3: Software management # I decided to try to use Coolify to manage what systems I would be running on the server. Had to rnn the install script a few times, but it was worth it cause it sends jokes like this:\nYour momma is so fat, you need to switch to NTFS to store a picture of her.\nWhen it installed I saved data from /data/coolify/source/.env to my password manager. Then I accessed the dashboard to create an account. Configured the notifications with Telegram via BotFather over a group chat.\nStep 4: Dynamic DNS # My ISP (Vergin Media IE) is providing IPv6 prefix leases. Once the lease expires I can get a new prefix which will cause the connected device to get a new public address.\nI\u0026rsquo;m going to leverage Cloudflare as my DNS provider and follow this guide to configure a Cloudflare tunnel. This way I get easy TLS, don\u0026rsquo;t have to worry about making my IP public, and by using a wildcard domain I can dynamically create service domain without editing the DNS table each time.\nStep 5: Single Sign On # I want to protect my services with SSO using Authentik. It\u0026rsquo;s easy to add it with Coolify. I used Mailersend to get SMTP service for sending emails. Mailersend easily configured my DNS records on Cloudflare.\nA thing to note here is that once a TXT DNS record for email was created on a subdomain, it no longer falls under the wildcard and I had to manually add a CNAME record for it and point it at the tunnel.\nA second thing to note is that Authentik populates the protocol (http vs https) in links in the page content, so while the Cloudflare was upgrading me to HTTPS on the top level the browser was then blocking requests to API endpoints. To mitigate this I had to follow this doc and create a HTTPS rule in the tunnel just for the specific subdomain, as well as change service to listen on https://*:9000 to trick proxy to into using http port 9000 while using https for resources in HTML.\nTODO: setting up flow for users\nStep 6: Monitoring # I found out about Victoria Metrics and decided to try and use it for me home lab monitoring.\nI used Docker compose environment for VictoriaMetrics to set up the metrics, logs and Grafana.\nTODO: Describe the setup once we figure out the proxy issue with Grafana\nSetup: /homeserver/telemetry\nExtra 1: DNS + DHCP # I wanted to try running AdGuard Home to proxy DNS requests and allow blocking some ads domains and potentially adult content (for when my baby becomes a child). It was very easy to install with Coolify, but then the challenge was to get all devices on my network to connect to it.\nBecause I have a Virgin Media Hub router with no setting for DNS, I had to resort to disabling DHCP on the router and instead running my own server. Luckily that\u0026rsquo;s built into AdGuard.\nWhat I had to figure out is to create a docker network using macvlan driver which would make my container become part of the outer network of my router and start responding to DHCP requests.\ndocker network create -d macvlan \\ --subnet=192.168.0.0/24 \\ --gateway=192.168.0.1 \\ -o parent=eth0 \\ local_macvlan services: adguard: image: adguard/adguardhome:latest restart: unless-stopped volumes: - ${WORK_DIR}:/opt/adguardhome/work - ${CONF_DIR}:/opt/adguardhome/conf ports: - \u0026#34;53:53/udp\u0026#34; #DNS - \u0026#34;53:53/tcp\u0026#34; - \u0026#34;67:67/udp\u0026#34; #DHCP - \u0026#34;68:68/udp\u0026#34; #DHCP - \u0026#34;68:68/tcp\u0026#34; networks: local_macvlan: ipv4_address: 192.168.0.2 # static IP networks: local_macvlan: name: local_macvlan external: true And it works very nicely. I kept IPv6 stateless distribution within the router, while IPv4 and DNS location is provided by my server.\n"},{"id":3,"href":"/excos/01-simple-cd-pipeline/","title":"1. Simple CI/CD pipeline","section":"Excos","content":" 1. Simple CI/CD pipeline # Before I start writing code for a new project I want to set up a very simple pipeline for getting it deployed. I decided to use Docker containers as means of packaging the app since they offer a lot of simplicity once built. If I was using a cloud platform for running containers it would likely come with some deployment system and a GitHub integration. But I want to keep this project fairly low cost for now. Instead of using a big cloud provider I\u0026rsquo;m renting a small VPS for 8 euro a month.\nA small server with a static IP address running docker. A basic setup for starting out. I tried googling container orchestration and everything comes with a lot of upfront configuration. Except docker-compose which has small enough footprint for my needs and I\u0026rsquo;ve worked with it before enough to be able to set something up within an hour. Docker commands can be executed over an SSH connection.\nAnother thing is dealing with docker images. I could use a container registry for storing the images, but if I want to iterate quickly I may end up creating a lot of images. Depending on the registry this comes with adequate costs, so I\u0026rsquo;m thinking of using it only for releases (however that gets defined later) while managing the short term images directly on the target host.\nPreparing the image # I\u0026rsquo;m going to skip over the building process for now. The important part is that we end up with a tagged image on our build machine. Ideally we want to assign a unique build number to the image each time we build the software. Same build number should be available to the application within the image to be later used for telemetry.\nWe want to follow the Semantic Versioning and one of the options is doing something like MAJOR.MINOR.PATCH-BUILD+SHA for continuously building the image (e.g. in a PR or feature branch) and dropping the build info once the change is accepted (e.g. merged into main). We can leverage something like GitVersion to automate the version calculation.\nUploading the image # In order to upload the image to the target host we will leverage the docker save and docker load commands. They work with a tarball and need an extra program to compress the image in transit (e.g. gzip). We will also leverage docker\u0026rsquo;s integration with SSH. The command will be as follows ( ref):\ndocker save \u0026lt;image:tag\u0026gt; | gzip | DOCKER_HOST=ssh://\u0026lt;user\u0026gt;@\u0026lt;host\u0026gt; docker load In order for this to work the user has to have access to the docker socket. However, having access to the docker socket means being able to ask docker engine to execute anything (as root).\nRunning containers with compose # We can leverage the DOCKER_HOST (or docker contexts) to remotely invoke the docker compose command and specify the configuration file with the -f parameter.\nscp docker-compose.yml \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;path\u0026gt; DOCKER_HOST=ssh://\u0026lt;user\u0026gt;@\u0026lt;host\u0026gt; docker compose up -d -f \u0026lt;path\u0026gt; Restricting access # Docker engine is running under the root account which means it\u0026rsquo;s able to do anything on the system. In particular a user with docker access could run a container and mount the filesystem root of the host in the container. There\u0026rsquo;s really a lot of space for exploitation and the general advice is to only give access to trusted users. However, since I\u0026rsquo;m going to give the CI system (e.g. GitHub Actions) the SSH key to access my server, I want to ensure there\u0026rsquo;s not too much an attacker could do should they obtain the secret key.\nI\u0026rsquo;ve been looking online and there\u0026rsquo;s generally not much on this topic. Majority of articles gives solid advice on how to isolate containers from one another or from the host. Example lists: OWASP, SecurityArchitect.\nFrom that second list I stumbled over this article on the use of AppArmor which does mention you can apply a restrictive policy to the docker engine process as well ( template). I might need to learn a bit more about AppArmor as it\u0026rsquo;s a solid security mechanism.\nContainers # Another set of things we can do is modify the Docker daemon configuration to set some good defaults for the containers.\nNote that any docker configuration can be overridden when running a command and this does not constitute a security measure!\n\u0026quot;userns-remap\u0026quot;: \u0026quot;default\u0026quot; - makes root user inside a container an unprivileged user when interacting with the host \u0026quot;no-new-privileges\u0026quot;: true - prevent the container from gaining new privileges via setuid or setgid binaries \u0026quot;icc\u0026quot;: false - disable inter-container connectivity over docker0 bridge network (requires creating custom networks) Additionally we can install sysbox and set the runtimes and default-runtime properties in the daemon config to use it.\nLeveraging sudoers # Another option I was looking at was to skip giving a user direct access to the Docker socket and instead use a sudoers configuration to provide a set of elevated commands the user can execute. This means that in the earlier examples instead of using SSH through the Docker client we will use SSH directly.\ndocker save \u0026lt;image:tag\u0026gt; | gzip | ssh \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt; sudo docker load # and then scp docker-compose.yml \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;path\u0026gt; ssh \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt; sudo docker compose up -d -f \u0026lt;path\u0026gt; For this we will create a new file under /etc/sudoers.d/\u0026lt;user\u0026gt; for our specific user:\n\u0026lt;user\u0026gt; ALL=(root:docker) NOPASSWD: /usr/bin/docker load \u0026lt;user\u0026gt; ALL=(root:docker) NOPASSWD:SETENV: /usr/bin/docker compose up -d -f * This means the user can impersonate only the root user under the docker group, they will not be asked for password and they are allowed to execute the specified commands. The SETENV flag allows us to pass environment variables with sudo -E which can be useful for parameterized compose files. Note that * means any string, so the user can provide extra parameters to the compose command apart from just the file. To restrict it further we should create a shell script which will validate arguments passed to it and apply them correctly. The script could also check that the docker-compose.yml is not making the containers run with higher privileges than we want.\nAfterwards verify the config with sudo visudo -c and you may have to run sudo chmod 0440 /etc/sudoers.d/\u0026lt;user\u0026gt; to restrict access to this file.\nExample limiting script # sudo curl -L -o /usr/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 sudo chmod +x /usr/bin/yq #!/bin/bash # docker-compose-up.sh RUNTIMES=$(yq \u0026#39;.services.*.runtime\u0026#39; ./docker-compose.yml) if [[ \u0026#34;$RUNTIMES\u0026#34; != null* ]]; then echo -e \u0026#34;The docker-compose file tries to override the default runtime\\n$RUNTIMES\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi docker compose -f $1 up -d Update: Adding --wait --wait-timeout 180 to the last command can allow you to wait until all containers are healthy or fail deployment.\nDisk space cleanup # As we are pushing images onto the server we need to be conscious of how much space they take. You should strive for building small container images with the minimal set of required dependencies, but even then they can add up and clog up the disk. Therefore you should look to clean up previous images after deployment.\nThe docker prune command can be used to delete unused resources. If you\u0026rsquo;re versioning the images such that they contain build number, rerunning CI on a previous commit will anyways create a new image so you can safely remove anything older. But if you want to keep the last N images for easy rollback to a previous version:\ndocker rmi $(docker images -q \u0026lt;repository/image\u0026gt; | tail -n +\u0026lt;N+1\u0026gt;) The name passed in doesn\u0026rsquo;t contain the tag, but starts with the repository (if not from Docker Hub). The images command returns images in order, latest first. The tail command prints lines starting from X if we pass -n X, so to skip N latest images we pass N+1.\nFull script (which takes 1 parameter, if not deletes all images):\n#!/bin/bash OLD_IMAGES=$(docker images -q $1 | tail -n +4) if [ -n \u0026#34;$OLD_IMAGES\u0026#34; ]; then docker rmi $OLD_IMAGES fi # remove dangling images docker image prune -f Conclusion # There we have it - a simple CI/CD pipeline which pushes a container image directly into the target host and uses docker compose to instantiate, while keeping in mind the security aspect of allowing external access to your server.\nOnce I have this up and running on GitHub I will add here a link to the workflow definition.\nExtra: Package update # For the CI portion of this, you should look to keep your dependencies up to date. You can use something like Renovate or at least Dependabot or set up your own thing.\nFor example I used dotnet-outdated in a simple GitHub action to update my package dependencies an create PR on a weekly basis.\nBtw, I recommend watching this talk: CI/CD antipatterns\n"},{"id":4,"href":"/stride/01-nuget-plugins/","title":"1. Using NuGet packages as plugins","section":"Stride","content":" 1. Using NuGet packages as plugins # This post will be describing my journey to get a sensible plugin orchestration and delivery mechanism in place for the Stride game engine. My main goal is to use NuGet architecture as much as possible and deliver a user experience that doesn\u0026rsquo;t require you to be a genius, nor has you copy-pasting things around too much. And because plugins are only meant for build time, and may target something else than your game, they cannot be directly referenced the usual way.\nStride integrates with MSBuild in a few ways. The GameStudio is acting a bit like Visual Studio in that it allows you to add projects to your solution, builds the projects for you when you make code changes, etc. So when thinking about a plugin architecture I immediately thought about using NuGet package references as a way to deliver plugins and resolve their dependencies. Stride already performs NuGet restore and parses the package.assets.json file to browse the dependencies of your projects. What is needed now is a system that allows us to detect plugins and load them into memory of the editor.\nDetecting something is a plugin # Initially I thought it may be enough to have the user mark a PackageReference with StridePlugin property. However, this requires the user to explicitly know what is a plugin and what isn\u0026rsquo;t. And there\u0026rsquo;s an issue with my ease-of-use goal. If the user references a runtime package, say Stride.Physics, we would expect that the plugin package Stride.Physics.Design would be automatically referenced as it is necessary to include it to process physics assets.\nSo I went online and I found PackageType concept which enables us to declare something is a Stride plugin when creating the NuGet package. It\u0026rsquo;s great and it also gives us the ability to filter NuGet.org for packages with the specified type - e.g. https://www.nuget.org/packages?packageType=StridePlugin - which is a huge win for a mechanism of discovering plugins.\nThis is great, however, currently package type is only used by NuGet SDK when installing package first time into your project to detect that for example your not referencing a .NET tool as dependency and it doesn\u0026rsquo;t preserve this information for later. So I have opened an issue on NuGet repo to ask for adding support for it ( #12934). In the meantime I foresee a potential workaround where a target in Stride.Core checks PackageType of your project and if it matches the plugin type it emits a new file into your package for which we can look after your package is unpacked in the nuget cache.\nOk, so given a dependency tree where some deps are plugins we can probably detect the plugins and select them for loading.\nCircular dependencies # So I mentioned earlier how the author of Stride.Physics should be able to declare that plugin package Stride.Physics.Design needs to be included for the user. How do the dependencies look like?\nUser project -\u0026gt; Stride.Physics -?-\u0026gt; Stride.Physics.Design -\u0026gt; Stride.Physics Ok, so in 90% of plugins I\u0026rsquo;m aiming at, there will be a runtime lib and a design lib and the design lib has to reference runtime lib to generate data for the runtime lib to consume.\nNuGet doesn\u0026rsquo;t allow circular dependencies. You won\u0026rsquo;t even create the package (if using regular tooling).\nSo I started thinking. And the idea was good but ultimately doesn\u0026rsquo;t work. But this blog post is about exploring ideas so let\u0026rsquo;s go.\nIndirect reference idea # Let\u0026rsquo;s simplify names - X is runtime lib, X.D is design time lib.\nX.D references X via ProjectReference which will be converted into a package dependency when packed. X.D marks itself as a Stride plugin. X notes \u0026lt;RequiredStridePlugin Include=\u0026quot;Plugin\u0026quot; Version=\u0026quot;1.0.0\u0026quot; /\u0026gt; in its project When X is packed we create a build/X.targets file which will be included in the user project that declares items IncludeStridePlugin matching the requirement. When user project referencing X is built we convert IncludeStridePlugin into PackageReference I learned a bit more about MSBuild while working on this. Especially trying to answer the question how to select the maximum version from the possibly duplicate containing list of IncludeStridePlugin items. And the answer was target batching.\nHere\u0026rsquo;s my code which was placed in Stride.Core targets:\n\u0026lt;!-- First the creator of a plugin will include the following in their runtime project: \u0026lt;ItemGroup\u0026gt; \u0026lt;RequiredStridePlugin Include=\u0026#34;MyPlugin\u0026#34; Version=\u0026#34;1.0.0\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; This target will generate a RequiredStridePlugin.targets file in the buildTransitive folder of the package, so that the consumers of the runtime package will have the following added to their project: \u0026lt;ItemGroup\u0026gt; \u0026lt;IncludeStridePlugin Include=\u0026#34;MyPlugin\u0026#34; Version=\u0026#34;1.0.0\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; --\u0026gt; \u0026lt;Target Name=\u0026#34;_StrideConfigureRequiredPluginsInPackage\u0026#34; BeforeTargets=\u0026#34;Build\u0026#34; Condition=\u0026#34;@(RequiredStridePlugin-\u0026gt;Count()) \u0026amp;gt; 0\u0026#34;\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;_StrideRequiredPlugin Include=\u0026#34;@(RequiredStridePlugin)\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;WriteLinesToFile File=\u0026#34;$(IntermediateOutputPath)$(PackageId).targets\u0026#34; Lines=\u0026#34;\u0026amp;lt;Project\u0026amp;gt;;@(_StrideRequiredPlugin-\u0026gt;\u0026#39;\u0026amp;lt;ItemGroup\u0026amp;gt;\u0026amp;lt;IncludeStridePlugin Include=\u0026amp;quot;%(Identity)\u0026amp;quot; Version=\u0026amp;quot;%(Version)\u0026amp;quot; /\u0026amp;gt;\u0026amp;lt;/ItemGroup\u0026amp;gt;\u0026#39;);\u0026amp;lt;/Project\u0026amp;gt;\u0026#34; Overwrite=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;!-- TODO: how can we allow the user to add their own targets? Using PackageId as name of the file is required for it to be loaded. see https://github.com/NuGet/docs.microsoft.com-nuget/blob/main/docs/reference/errors-and-warnings/NU5129.md --\u0026gt; \u0026lt;None Include=\u0026#34;$(IntermediateOutputPath)$(PackageId).targets\u0026#34; PackagePath=\u0026#34;build\\\u0026#34; Pack=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;None Include=\u0026#34;$(IntermediateOutputPath)$(PackageId).targets\u0026#34; PackagePath=\u0026#34;buildTransitive\\\u0026#34; Pack=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Target\u0026gt; \u0026lt;!-- Based on IncludeStridePlugin declared we will try to load the latest version of the plugin. This only matters if two referenced library require the same plugin but with different versions. \u0026lt;IncludeStridePlugin Include=\u0026#34;X\u0026#34; Version=\u0026#34;1.2.0\u0026#34; /\u0026gt; \u0026lt;IncludeStridePlugin Include=\u0026#34;X\u0026#34; Version=\u0026#34;1.4.0\u0026#34; /\u0026gt; becomes \u0026lt;_StridePluginReference Include=\u0026#34;X\u0026#34; Version=\u0026#34;1.4.0\u0026#34; /\u0026gt; --\u0026gt; \u0026lt;!-- We use the Inputs and Outputs to allow Target batching and group IncludeStridePlugin by package name --\u0026gt; \u0026lt;Target Name=\u0026#34;_StrideIncludePluginReferencesByHighestVersion\u0026#34; BeforeTargets=\u0026#34;CollectPackageReferences\u0026#34; Inputs=\u0026#34;@(IncludeStridePlugin)\u0026#34; Outputs=\u0026#34;__%(Identity)\u0026#34;\u0026gt; \u0026lt;!-- for each Version update _TempStridePluginVersion if Version \u0026gt; _TempStridePluginVersion --\u0026gt; \u0026lt;CreateProperty Value=\u0026#34;%(IncludeStridePlugin.Version)\u0026#34; Condition=\u0026#34;\u0026#39;$(_TempStridePluginVersion)\u0026#39; == \u0026#39;\u0026#39; OR $([System.Version]::Parse(%(IncludeStridePlugin.Version))) \u0026amp;gt; $([System.Version]::Parse($(_TempStridePluginVersion)))\u0026#34;\u0026gt; \u0026lt;Output TaskParameter=\u0026#34;Value\u0026#34; PropertyName=\u0026#34;_TempStridePluginVersion\u0026#34; /\u0026gt; \u0026lt;/CreateProperty\u0026gt; \u0026lt;!-- include the plugin reference with the greatest version --\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;_StridePluginReference Include=\u0026#34;%(IncludeStridePlugin.Identity)\u0026#34; Version=\u0026#34;$(_TempStridePluginVersion)\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;!-- unset property for the next target invocation --\u0026gt; \u0026lt;CreateProperty Value=\u0026#34;\u0026#34;\u0026gt; \u0026lt;Output TaskParameter=\u0026#34;Value\u0026#34; PropertyName=\u0026#34;_TempStridePluginVersion\u0026#34; /\u0026gt; \u0026lt;/CreateProperty\u0026gt; \u0026lt;/Target\u0026gt; \u0026lt;!-- For each _StridePluginReference we will add a PackageReference if not already set by the user (allows user to override the version). Needs to be invoked before CollectPackageReferences target for the packages to be picked up for build correctly. We only include build assets from the package to avoid the plugin being used for compilation. --\u0026gt; \u0026lt;Target Name=\u0026#34;_StrideIncludePluginReferencesFromPackages\u0026#34; AfterTargets=\u0026#34;_StrideIncludePluginReferencesByHighestVersion\u0026#34; BeforeTargets=\u0026#34;CollectPackageReferences\u0026#34; Condition=\u0026#34;@(_StridePluginReference-\u0026gt;Count()) \u0026amp;gt; 0\u0026#34;\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;@(_StridePluginReference)\u0026#34; Exclude=\u0026#34;@(PackageReference)\u0026#34; IncludeAssets=\u0026#34;build;buildTransitive\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Target\u0026gt; So why doesn\u0026rsquo;t it work?\nWell, the issue is simple - you can\u0026rsquo;t run targets from other packages during Restore because they\u0026rsquo;re not restored yet. And if you cannot add the PackageReference before the restore then it\u0026rsquo;s not restored, it\u0026rsquo;s not in the dependency graph, not populated in project.assets.json and not taken into account during build.\nMoreover, I found this\nThere are a few things that must not be done in packages\u0026rsquo; .props and .targets, such as not specifying properties and items that affect restore, as those will be automatically excluded.\nSome examples of items that must not be added or updated: PackageReference, PackageVersion, PackageDownload, etc. So it looks like we probably can\u0026rsquo;t get away with indirectly including package references just yet.\nMaking plugins not have dependencies # Something I haven\u0026rsquo;t considered too much but crossed my mind - what if we enforce PrivateAssets=all on any dependency of the plugin, thus making it not have dependencies at all, thus none of them will be circular? Will need to think more.\nNext? # This post will be updated over time. For the main issue spawning these ideas look at Stride Plugin RFC.\n"},{"id":5,"href":"/iqa-management-hub/01-writing-system-tests/","title":"1. Writing system tests","section":"IQA Management Hub","content":" 1. Writing system tests # So here I am - I made a decision to make a rewrite of the service. Where do I start? Well, the number one thing I want to do is to try to follow good practices. I don\u0026rsquo;t want to just write this thing from scratch and swap them out in one go, possibly breaking tens of existing users with something I haven\u0026rsquo;t tested. So I need to first document how the existing service works. I will use system tests for that.\nWhat are system tests? # If you heard about the test pyramid you may be familiar with service or integration tests. The way I define system tests is an external testing process making calls to a locally deployed application and validating the general behavior.\nWhat this means is that the system test mostly doesn\u0026rsquo;t care about implementation details and focuses on externalities. For a web API that means most tests should be performable by making HTTP requests against the service. In my case because I need to document a bit deeper how the service behaves I will also be connecting to the database to inspect it, clean up state created with the tests, etc.\nSetup # I\u0026rsquo;ve created a new .NET 6 project with the XUnit template and added Xunit.DependencyInjection which allows me to use dependency injection, configuration files and advanced logging controls.\nI\u0026rsquo;ve also created a project for models and generated classes using Entity Framework Core dotnet tool. The app uses postgres so I installed Npgsql.EntityFrameworkCore.PostgreSQL. Then I ran dotnet ef dbcontext scaffold to set up the models. I like to keep models mostly pure, so all my EF configuration ends up in the DbContext class.\nFinally I needed some tools for making the HTTP requests and I opted for writing my own, specifically tailored to this app.\nHttpClient # I have read a lot in the past on how to use the HttpClient. The most recent advice says to use IHttpClientFactory from the Microsoft.Extensions.Http package. It allows us to use dependency injection to configure the client. And you can name the client to have multiple configurations.\nBut the main benefit is that the factory intelligently caches the HttpClientHandler instances which hold system resources. A big issue of manual handler management is port exhaustion when you create to many instances. It\u0026rsquo;s a general issue and I\u0026rsquo;ve seen systems die from this.\nIn .NET Core a new system of layers has been designed on top of the HttpClientHandler. There\u0026rsquo;s many small and reusable instances of DelegatingHandler which act very similarly to ASP.NET middleware. It can be used for injecting headers or otherwise modifying the request or the response.\nCookies # For accessing the management hub website you login with an email and a password and get a session cookie. Standard stuff. Took me 2 days to make it work.\nSo the first thing is cookies. As mentioned above, the HttpClientHandler is reused by multiple HttpClient instances to save on system resources. This means that the built in cookie management mechanism are out the window, because they\u0026rsquo;d be shared across multiple clients. We need the cookie store to be specific to a given test. Also we need to allow tests to run in parallel for fast CI times later on.\nSo I looked around and came up with this middleware (shout out to damianh\u0026rsquo;s gist which saved me from parsing cookies by hand)\n/// \u0026lt;summary\u0026gt; /// Message handler which substitutes the handling of cookies of the \u0026lt;see cref=\u0026#34;HttpClientHandler\u0026#34;/\u0026gt;. /// It allows to reuse a single instance of a underlying handler across multiple cookie sessions. /// \u0026lt;/summary\u0026gt; public class CookieSessionMessageHandler : DelegatingHandler { public static readonly HttpRequestOptionsKey\u0026lt;CookieContainer\u0026gt; CookieContainerOption = new(\u0026#34;CookieContainer\u0026#34;); protected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken) { if (!request.Options.TryGetValue(CookieContainerOption, out var cookieContainer)) return await base.SendAsync(request, cancellationToken); if (cookieContainer.Count \u0026gt; 0) request.Headers.Add(\u0026#34;Cookie\u0026#34;, cookieContainer.GetCookieHeader(request.RequestUri!)); var response = await base.SendAsync(request, cancellationToken); if (response.Headers.TryGetValues(\u0026#34;Set-Cookie\u0026#34;, out var cookieValues)) foreach (var cookieValue in cookieValues) { cookieContainer.SetCookies(request.RequestUri!, cookieValue); } return response; } } I\u0026rsquo;ve cleaned up the code a bit for this post to keep it shorter, but I also have some debug logs in there.\nWe can see I\u0026rsquo;m using the HttpRequestOptions collection to store the CookieContainer. This means I needed to write custom extensions over HttpClient for things like GetStringAsync(), because I wanted to build my own HttpRequestMessage and add the cookie container.\nThe middleware also works if we don\u0026rsquo;t care about cookies - this is generally a good practice - make your middleware opt-in rather than opt-out.\nFor sending the cookies we\u0026rsquo;re using a method of the CookieContainer called GetCookieHeader(url). For saving new cookies we\u0026rsquo;re using the SetCookies(url, cookieHeader) method. If you happen to have issues with it, checkout the helper class SetCookieHeaderValue, from Microsoft.Net.Http.Headers package, to parse the headers.\nSo with this cookie middleware I was able to log in, but afterwards something wasn\u0026rsquo;t working and I had to debug it.\nRedirects # The session cookie system of Rails works in a way that sends a new cookie with each request. I believe it\u0026rsquo;s meant to prevent an attacker from stealing a session in some way (see Rails guide on security).\nWith this new cookie middleware I was sending redirect requests with the same cookie as the original request.\nsequenceDiagram CookieMiddleware-\u003e\u003eHttpClientHandler: Headers[\"Cookie\"] HttpClientHandler-\u003e\u003eWebsite: HTTP request Website--\u003e\u003eHttpClientHandler: 302 Headers[\"Location\"] HttpClientHandler-\u003e\u003eWebsite: HTTP GET request to new location Website--\u003e\u003eHttpClientHandler: 200 OK HttpClientHandler--\u003e\u003eCookieMiddleware: Headers[\"Set-Cookie\"] And this wasn\u0026rsquo;t working, so I had to write my own redirect layer on top of it.\n/// \u0026lt;summary\u0026gt; /// Message handler which substitutes the handling of redirects of the \u0026lt;see cref=\u0026#34;HttpClientHandler\u0026#34;/\u0026gt;. /// We needed redirection layer on top of the custom cookie middleware. /// \u0026lt;/summary\u0026gt; public class FollowRedirectsMessageHandler : DelegatingHandler { protected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken) { var response = await base.SendAsync(request, cancellationToken); if (response.Headers.TryGetValues(\u0026#34;Location\u0026#34;, out var locations)) { var redirectRequest = new HttpRequestMessage(HttpMethod.Get, locations.First()); if (request.Options is IDictionary\u0026lt;string, object?\u0026gt; requestOptions \u0026amp;\u0026amp; redirectRequest.Options is IDictionary\u0026lt;string, object?\u0026gt; newOptions) foreach (var kvp in requestOptions) { newOptions.Add(kvp.Key, kvp.Value); } return await this.SendAsync(redirectRequest, cancellationToken); } return response; } } Basically whenever there\u0026rsquo;s a Location header present we make a new request there and copy all the request options. Note how I used base.SendAsync and this.SendAsync to go directly down the request pipeline or call this method recursively to allow for following multiple redirects.\nsequenceDiagram RequestMiddleware-\u003e\u003eCookieMiddleware: . CookieMiddleware-\u003e\u003eHttpClientHandler: Headers[\"Cookie\"] HttpClientHandler-\u003e\u003eWebsite: HTTP request Website--\u003e\u003eHttpClientHandler: 302 Headers[\"Location\"] HttpClientHandler--\u003e\u003eCookieMiddleware: Headers[\"Set-Cookie\"] CookieMiddleware--\u003e\u003eRequestMiddleware: . RequestMiddleware-\u003e\u003eCookieMiddleware: GET redirected CookieMiddleware-\u003e\u003eHttpClientHandler: Headers[\"Cookie\"] HttpClientHandler-\u003e\u003eWebsite: HTTP request Configuring the HttpClient # Finally we go to the Configure method to set up dependency injection.\nservices.AddTransient\u0026lt;FollowRedirectsMessageHandler\u0026gt;(); services.AddTransient\u0026lt;CookieSessionMessageHandler\u0026gt;(); services.AddHttpClient(RequestBuilder.HttpClientName) .ConfigurePrimaryHttpMessageHandler(() =\u0026gt; new HttpClientHandler { AllowAutoRedirect = false, UseCookies = false, }) // Ordering of the handlers is important, from top (outermost) to bottom .AddHttpMessageHandler\u0026lt;FollowRedirectsMessageHandler\u0026gt;() .AddHttpMessageHandler\u0026lt;CookieSessionMessageHandler\u0026gt;(); We disable redirects and cookies on the HttpClientHandler because we add middleware to handle it instead.\nEmails # Nowadays it feels like emails are a bit of a thing of the past, but at the same time they are everywhere. Products like SendGrid enable you to make a HTTP request to their server and they take care of sending the email. But without modern solutions it is still possible to send email over the basic SMTP protocol. And that is exactly what Management Hub is doing.\nUsually you could write the email to memory or the file system for testing. But to have additional confidence in my system I want to make sure the mailing library I use is able to send the email properly over SMTP.\nSo in my tests I included the library netDumbster - a simple SMTP server, which allows me to receive emails sent by the web service. In order to not have to run my tests with admin privileges I configured both the service and the library to work on port 4025.\nI\u0026rsquo;ve used the IHostedService interface on my wrapper around the SMTP server to start it once for the whole testing process.\nservices.AddSingleton\u0026lt;EmailProvider\u0026gt;(); // auto-start email server services.AddSingleton\u0026lt;IHostedService\u0026gt;(sp =\u0026gt; sp.GetRequiredService\u0026lt;EmailProvider\u0026gt;()); Then the test receives the wrapper from the DI framework and checks the incoming messages (in a polling fashion in case there\u0026rsquo;s a delay between HTTP response and email being sent) for the one matching a predicate.\npublic async Task\u0026lt;SmtpMessage\u0026gt; PollAsync(Func\u0026lt;SmtpMessage, bool\u0026gt; predicate, TimeSpan? timeout = null) { timeout ??= TimeSpan.FromSeconds(10); using var cts = new CancellationTokenSource(timeout.Value); while (true) { Assert.False(cts.Token.IsCancellationRequested, \u0026#34;Smtp polling timeout\u0026#34;); var message = this.emailMessages.SingleOrDefault(predicate); if (message != null) return message; await Task.Delay(PollingInterval); } } Unique persistent users # As I started writing the tests I decided it\u0026rsquo;s a good idea to have each test go through a full cycle of setup, assertion and cleanup. This means creating a new user, setting up new objects in the database, executing some actions, and finally removing all created things from the database. I found it an issue to give all tests the same email address, because they can run in parallel and I wouldn\u0026rsquo;t want them to mess with one another. But random ids are also bad, because they can lead a test to fail some of the time if a bad id is causing an issue. Or in my case, if a test fails and there\u0026rsquo;s a bug in the cleanup code, having a persistent id can help execute the cleanup at the beginning of the next test run.\nI used a simple mechanism of having a helper method which takes a string argument with [CallerMemberName] which is the name of the test method, an integer seed (in case one test needs multiple users) and returns a user context object, which implements IDisposable to perform cleanup.\nFrom the name of the method I calculate a stable hash (don\u0026rsquo;t use GetHashCode - it is stable only within a single run) using the FarmHash64 algorithm from FastHashes and converting the result to a Base64 string (which makes a valid email username). Also needed to cast the email ToLower() because the service was configured with case insensitive email comparisons.\nprivate static string GenerateEmailFromString(string input, int seed = 0) { var inputAsByteSpan = MemoryMarshal.Cast\u0026lt;char, byte\u0026gt;(input.AsSpan()); // We\u0026#39;re using a stable hashing algorithm here to ensure // the same email is use for a given test between runs string hash = Convert.ToBase64String(HashAlgorithm.ComputeHash(inputAsByteSpan)); string seedStr = seed != 0 ? seed.ToString() : string.Empty; // Need to call ToLower, because Devise is doing that before saving the email to the database return $\u0026#34;user_{hash.ToLowerInvariant()}{seedStr}@example.com\u0026#34;; } "},{"id":6,"href":"/web/02-experimentation/","title":"2. Short guide to experimentation","section":"Web","content":" 2. Short guide to experimentation # Recently I\u0026rsquo;ve been involved in running an experiment at work and decided to dive a bit deeper into all the different parts of how to get it set up. You can find a lot of articles online which talk very vaguely about how to run an experiment. This post is meant to be a cheat sheet of sorts to give you a step by step of what is needed, but at the same time it won\u0026rsquo;t cover the topic very in depth. For the coding and instrumentation part I will focus on .NET as it\u0026rsquo;s my preferred tech stack but with a little bit of research you should have no problem finding the libraries for your preferred stack.\nWhat is experimentation and why do we care? # If I\u0026rsquo;m running a small website and decide to change something in the UI for example, do I care how it affects users it looks better to me? Probably no. But if I\u0026rsquo;m running a business and care about how many people execute some action on my site and what contributes to it then it may be a bit more important.\nSo say, you have a checkbox in a sign-up form where the user can opt-in to additional marketing communication. Right now it just has some basic text next to it which is there to announce what this checkbox is and some legally required formula. We want to make it look nicer by putting it on different background color and adding an envelope icon.\nThe question is, how is it going to affect the users. Are they going to click the checkbox more, or less? To answer the question we first have to measure this engagement somehow and then compare the numbers between old and new UI.\nWithout heading towards statistics too fast, let\u0026rsquo;s talk about a high level expectation for an experiment. We want to check if the change in UI affects the user action. What if there is other factors that affect the user action? Do users subscribe to mailing more if they fill out the form in the morning and less in the evening? Do users who\u0026rsquo;s name start with \u0026lsquo;M\u0026rsquo; are more into mailing offers? Or maybe there are small differences in how the page is rendered in different browsers on or mobile that would affect this?\nAn A/B test tries to compare two page variants where only 1 factor, our feature, is the differing factor. This way, if we can prove a statistically significant difference in the measured user action across the page variants, it means it wasn\u0026rsquo;t influenced by other factors and it\u0026rsquo;s only the feature that has an impact.\nWhat do we need to conduct this experiment? # Partition users into two equal groups (50/50). One group will see the old UI and one the new UI. Show the right UI variant to the given group. Log the signal of whether user interacted with the UI to a data store. Perform data analysis using statistical methods to make conclusions which UI is better at making users engage with it. Feature management # We need to put some code into our website which will check whether the feature is on or off, or whether to use value X or Y as part of the feature evaluation. The most basic thing you can do is just hardcode a constant and check if it\u0026rsquo;s value is true or false. And with each deployment of the website you change what is being shown.\nconst bool isNewUIEnabled = true; if (isNewUIEnabled) { //... } else { //... } Now, this has two downsides:\nYou can\u0026rsquo;t modify the value when the service is running. Depending on your set up the deployment can take a long time. You can\u0026rsquo;t present the 2 UI during the same time frame. For the experiment quality, you generally want to run your two variants at the same time. This way you can account for situations like I mentioned earlier - what if users are more into subscribing on Monday mornings. So let\u0026rsquo;s look at alternative systems.\nThe Microsoft.Extensions.Configuration solves the issue (1) by allowing you to specify config sources that can be modified at runtime. From simple json files to integrations with Azure, AWS and other configuration providers. There\u0026rsquo;s not much in the open source for fully fledged solutions here, but at the same time it\u0026rsquo;s not hard to build something yourself to suit your needs - example.\nBut we still need to solve for (2). There is a decent framework for .NET Microsoft.FeatureManagement and you can read about how to get started with it in this wonderful (but a little old) series by Andrew Locke. There are other solutions like this library with ASP.NET UI and now I\u0026rsquo;m looking forward for open source integrations with the new Microsoft.Extensions.Options.Contextual.\nI actually got inspired to write a library myself. Check out Excos.Options.\nRegardless of which lib you go with, you want to make your experiments sticky to a browser session or user identifier (for authenticated experiences) to ensure consistency (imagine the UI changes when you refresh the page - yuck!).\nYou want to end up with 50% of users getting the feature variant A and 50% getting the variant B. I recently learned it\u0026rsquo;s very important to have a very consistent 50/50 split. Otherwise you can get into a Signal Ratio Mismatch - which means more users see one of the variants than the other. What\u0026rsquo;s the big deal? It can mean that another factor is affecting the situation, and remember, we said we want our feature to be the only varying factor. There\u0026rsquo;s a cool paper on the topic.\nOnce you have code in place, the feature configured to be shown to the user groups, make sure it actually works and looks as expected before launching the experiment. A very useful thing is to have a feature flag override via query parameters or a cookie.\nTelemetry # I generally split the telemetry into three categories: diagnostics, health metrics, analytics. You use diagnostics to debug your service if somethings doesn\u0026rsquo;t work, you display health metrics on a dashboard to see if everything is working well, and with analytics logs you store them somewhere for further processing to generate business relevant metrics.\nFor experiments we focus on analytics logs and we will care about a few things:\nSession or User identifier - the unit of our measurements Where they selected for the experiment? Was it shown to them? Did they complete the action (or in general what actions did they execute in case the experiment affects some other action)? When did stuff happen? Other metadata to account for additional factors (e.g. country, browser, age group, etc) Make sure to prepare questions for your experiment upfront so that you know what needs to be collected. It\u0026rsquo;s really not great when you decide mid experiment you\u0026rsquo;re missing some additional context. Example questions:\nIf we show the new UI, does the rate of submitting the form change? If we show the new UI, do more people subscribe for mailing? If the user visited our site before in the last month, are they much more likely to subscribe with the new UI (bigger % difference in comparison to first time visitors)? Where do we put this telemetry? It will depend on your website\u0026rsquo;s traffic size and budget. There\u0026rsquo;s plenty Big Data storage solutions but the easiest is just to log it to a SQL database. Choose a flexible schema which allows you to add more analytics telemetry signals over time to build a rich picture of the user behavior.\nQuick note: you may need to inform your user that you\u0026rsquo;re collecting data and for what purpose. Experimentation and service improvement is generally acceptable under \u0026ldquo;required\u0026rdquo; data, while marketing personalization is usually optional and requires user consent. Be careful with user data. It\u0026rsquo;s a good idea to store analytics telemetry separately from your users\u0026rsquo; personal data.\nOnce you\u0026rsquo;ve collected \u0026lsquo;raw\u0026rsquo; telemetry you may want \u0026lsquo;cook\u0026rsquo; it, by transforming it a bit. Suppose we have one table for page views, one table for button clicks on page, one table for the end state of submitted forms. You would aggregate those to get a single row for the user/session with all the context: [who, page, variant, submitted, xBtnClickCount, xFinalState].\nAnalysis # You can do some basic analysis yourself with ad hoc queries and often you will get a decent picture of how the experiment went, but there\u0026rsquo;s a lot of statistics and complexity that needs to be accounted for in order to \u0026ldquo;get it right\u0026rdquo;. One way is to learn more about Data Science, another is to find existing solutions which can connect to your database and give you the insights.\nOne of the nice open source projects I found is GrowthBook. It comes with a statistics engine for data analysis. It also has feature flag management, but through its custom SDK instead of the popular .NET frameworks (I might try to integrate it with contextual one day). It can be self hosted with Docker.\nRead more # Practical Guide to Controlled Experiments on the Web O\u0026rsquo;Reilly - Experiment!: Website conversion rate optimization with A/B and multivariate testing The Open Guide to Successful AB Testing Growth Book Bayesian Statistics Engine Microsoft Experimentation Platform "},{"id":7,"href":"/excos/03-privacy-compliance/","title":"3. Implementing privacy compliance","section":"Excos","content":" 3. Implementing privacy compliance # In the modern internet everyone and their mother is collecting data about users. And to be fair, I also want to collect some for legitimate reasons - making a better product and a better experience to my users. But because there\u0026rsquo;s a lot of nefarious actors out there we now have regulations in place that explicitly require services to provide users with more control over their data.\nGDPR and ePrivacy directive are the EU legislation which mandate consumer privacy rights. Their existence has spawned such services as Plausible and Umami which offer compliant web analytics.\nThis blog will try to cover how I\u0026rsquo;m looking to implement privacy measures in Excos to comply with GDPR. Mind you, there\u0026rsquo;s no one way to do things compliantly, so depending on the requirements of your system this approach may or may not be suitable.\nLegitimate interest \u0026amp; Consent # When it comes to collecting and processing personal data (which includes anything that could identify an individual) we either need to show that we have a legitimate interest, i.e. the collection is required for what the user is trying to accomplish using the service, or we need to gather consent from the user for the \u0026ldquo;extra\u0026rdquo; data use.\nExcos is a platform providing remote application configuration, experimentation over application telemetry and decision automation. Collecting user billing details for example would be a legitimate interest to enter into a contract with the user to provide them the services.\nBut often we are collecting personal data without the user explicitly providing us with it. For instance, the country inferred from user\u0026rsquo;s IP address for service telemetry can be considered a personal data point if it\u0026rsquo;s stored next to a user identifier. And often times we want to know how many unique users we deal with so having some sort of user identifier is necessary to calculate it.\nDo we need to calculate unique users to provide the service? Generally, no.\nSo if we want to do processing of personal data that is not the required to provide a service to the user (that they want to be provided), we need to get their consent. Let\u0026rsquo;s look at the following requirements:\nConsent must be freely given, specific, informed and unambiguous. The data subject must also be informed about his or her right to withdraw consent anytime. The withdrawal must be as easy as giving consent. The consent must be bound to one or several specified purposes which must then be sufficiently explained. Consent cannot be implied and must always be given through an opt-in, a declaration or an active motion. The controller is not allowed to switch from the legal basis consent to legitimate interest once the data subject withdraws his consent. This applies even if a valid legitimate interest existed initially. Therefore, consent should always be chosen as a last option for processing personal data. source Let\u0026rsquo;s say we\u0026rsquo;re considering the following:\nCollect personal data to provide the service: We store the information on which user made a change to a configuration and we may display this information on the configuration page to other users in the same organization. We store the user email address in order to be able to send them a notification they opt-into once the system detects a problem with the user\u0026rsquo;s application. Collect personal data to allow for debugging and customer support: We store information on the user action in the telemetry so that in case of a system error we can trace back the user steps and reproduce them to understand and fix the issue. Collect and use telemetry to understand product usage and improve the product: We may collect some extra telemetry to paint a bigger picture of user behavior, which was not needed for debugging only. Create a user profile for experience personalization: We create a user profile that tells us about their product usage history which we use to alter the user experience via in-app suggestions. Create a user profile for marketing purposes: We create a user profile in order to target them with marketing messages, such as to upsell freemium users. Create an audit log for security purposes: We store the user details together with information on when they signed in or accessed a resource in the system. If we detect suspicious activity we may temporarily block the user account. We want to process as much data under legitimate interest clause as we can. From what I\u0026rsquo;ve read we can treat 3 of the above as such: 1. provide, 2. debug problems, 6. security.\nThen we would want to collect consent for the other 3 purposes. Each purpose is treated separately and must be opted-into by the user separately. For example, using check-boxes on the sign up form and in the user settings (remember about removing consent).\nI\u0026rsquo;m going to create a consent bit-map:\n0 - no consent for non-required data 1 - consent for product improvement telemetry (I) 2 - consent for personalization profile (P) 4 - consent for marketing profile (M) The bitmap value will be stored in the user settings and used when initializing the telemetry system, so that we can decide on whether to collect extra telemetry or not. Moreover, the telemetry will be stamped with the consent information to create evidence of what the user consent was when the data was collected. The user may later take back consent which means we can no longer process the previously collected data, but it doesn\u0026rsquo;t have to be deleted (immutable telemetry).\nThis last bit is important - it means that any time we perform processing we need to know what is the latest consent state. Since we\u0026rsquo;re tagging all our logs with the consent bitmap, it should be possible to do ARGMAX(time, consent_map) to find out their current preference and filter the rest of the logs accordingly.\nRight to be forgotten # Under the GDPR the user has the right to request removal of their personal data. They can request only specific data to be removed.\nFor this purpose we\u0026rsquo;re going to implement the following:\nUser has a persistent account ID (puid) This id is used for cases where despite data removal request the data will have to be retained for other legal reasons: e.g. security and billing (within some retention period). This id is used for account access, authorization, tenant membership, etc. Any identifiable information such as email, name, etc. is stored together with this ID. User has an operational user ID (oid) This id is used in events and databases to denote actions taken by the user. This id is tied to a user alias that they agreed can be shared publicly. Upon user closing their account, the connection between their persistent ID and operational ID is severed. User has at least one telemetry ID (tyid) This id is created by hashing the operational ID with a salt that the user can ask to be rotated, which de-links all previously collected for them telemetry. Resulting in a UUIDv5. This id is used in all telemetry instead of the operational ID, which preserves unique user semantics and facilitates debugging. Upon user closing their account the salt is cleared or rotated. In order to prevent indirect identification, any other object IDs stored in a way where they can be linked to an operational ID also need to be hashed before being added to telemetry with a tenant level salt.\nUser\u0026rsquo;s personal data, except where it needs to be kept for legal reasons should be erased.\nRight of access # The user has the right to request a copy of all data currently held on them by the system. They should have an \u0026ldquo;easy\u0026rdquo; way to download their data and they can request it over email or other channels.\nWe already are going to have an OData based web API for programmatic access, but we also need a button in the user profile where they can request their data easily. A database sourced export would cover anything available through the API, but in a simplified user-centric view. For example we wouldn\u0026rsquo;t export all the configuration fields the user has modified, but only the fact they they modified that specific config.\nThen a second data export would mainly consist of exporting the telemetry. But we don\u0026rsquo;t need to export all telemetry, but only that pertains to what we know about the user actions. If a user action results in multiple telemetry events describing the system\u0026rsquo;s internal operations, we only surface to the user the action that they had taken.\nTo support this, all telemetry events must be annotated with the information on whether they are exportable or not. All UX events directly initiated by the user are exportable.\nA data pipeline needs to be put in place where once daily/weekly (depending on the export event volume) it takes all the pending requests and executes a query over the telemetry and the database that reads all data we can currently link to the user and puts them into a portable file format the user can open in a blob storage. The user would be then emailed a secure link to access this file. After 30 days the file would be deleted.\nIn order to prevent abuse, a user cannot issue a new request until the previous one is completed.\nData classification # Dealing with personal data requires us to classify certain data fields to denote that what they contain is personal or can link to personal data. Further more most debugging activities should not allow us (the host) to view all data the users are managing. Since this is potentially a B2B service, we wouldn\u0026rsquo;t want to for example see their config names which could leak upcoming features before they are announced.\nAs such the data must be classified. We will leverage this package to create data classification attributes and ensure any sensitive data is redacted before logging.\nClassification:\nUII - User Identifiable Information This covers any directly identifiable information such as email, name, address, IP address (in some cases) UPI - User Pseudonymous Identifier This covers any identifier which could be indirectly used to reach UII such as user IDs, session IDs, object IDs (if those objects link to a single specific user ID) UDI - User Demographic Information This covers any data about the users age group, country of residence, or other demographic elements, which would not identify a single individual if de-linked CC - Customer Content Any content which the users upload to the system (e.g. config names) which may be sensitive OI - Organization Information Non personal data related to the tenant (including tenant ID) SYS - System Metadata Any data which is system specific and would not be considered personal nor can be used to link to personal data on its own (e.g. enums, certain object IDs) No UII and CC can appear in the logs. Only processed UPIs (appropriately hashed) can appear in logs.\n"},{"id":8,"href":"/excos/","title":"Excos","section":"Marian's Blog","content":" Excos # I\u0026rsquo;ve started a new project. The concept is as follows: services and clients need configuration. Changes in software version or in configuration version can have an impact on the observed metrics. We should be validating changes as we make them. Excos is a configuration, observability and experimentation platform which aims to enable safe software changes.\nIt\u0026rsquo;s going to be an open source project which could potentially be turned into a SaaS offering later on. To be notified of the development efforts subscribe on https://excos.dev\nAnd you can also check out my post on experimentation.\n"},{"id":9,"href":"/homelab/","title":"Homelab","section":"Marian's Blog","content":" Homelab # I decided to run a home server. The posts will go over my setup in case I need to reproduce it.\n"},{"id":10,"href":"/iqa-management-hub/","title":"IQA Management Hub","section":"Marian's Blog","content":" IQA Management Hub # You may not have heard about it, but there\u0026rsquo;s a sport called quadball, formerly known as quidditch. I discovered it when I was in highschool, got really into it. But obviously sport is not just the players. It\u0026rsquo;s referees, team managers, league organizers. So at some point I decided to volunteer for the IQA - International Quadball Association. And my favorite capacity of volunteering is as a software engineer.\nIQA doesn\u0026rsquo;t have a lot needs for a big team of programmers. They run a website with news blog posts and a service called Management Hub. It\u0026rsquo;s a single place for admins of National Governing Bodies (country members of IQA) to provide official statistics to the IQA and a referee hub for certifying referees.\nSo at the moment of writing I\u0026rsquo;m the one maintaining it.\nAnd as I set off in my role I decided I don\u0026rsquo;t want to do any rewrites, just keep the existing codebase alive. Easier said than done. It was written in ruby on rails and it\u0026rsquo;s cool. BUT I can\u0026rsquo;t spend the time to learn ruby in depth to migrate to a newer version where a lot of things will break. AND I like C# a lot. So I\u0026rsquo;m going to try to rewrite this site into ASP.NET Core and try to fix a lot of issues that piled up over time.\n"},{"id":11,"href":"/stride/","title":"Stride","section":"Marian's Blog","content":" Stride # I\u0026rsquo;m a contributor to the Stride game engine. It\u0026rsquo;s open source, it\u0026rsquo;s C# - I love it.\nStride is a huge code base with over 100 projects and many users all over the world with a variety of needs. My main area of contribution looks at core libraries, serialization system, asset pipeline and also the plugin architecture.\n"},{"id":12,"href":"/telemetry/","title":"Telemetry","section":"Marian's Blog","content":" Telemetry # I\u0026rsquo;m a big fan of observability of services, being able to say how they are performing and what are the issues by looking at dashboards and querying logs. In order to get that nice picture one needs to instrument their applications to emit telemetry signals - logs, metrics, traces.\n"},{"id":13,"href":"/web/","title":"Web","section":"Marian's Blog","content":" Web # At work we mainly focus on web services, so naturally I have some challenges in that space to write about. It will generally focus on the ASP.NET Core stack.\n"}]