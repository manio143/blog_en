[{"id":0,"href":"/telemetry/01-exporting-eventsource-locally/","title":"1. Exporting EventSource logs to CSV","section":"Telemetry","content":" 1. Exporting EventSource logs to CSV # Here\u0026rsquo;s a scenario - you\u0026rsquo;re building a service and you\u0026rsquo;re emitting logs as you should be. But your service is configured to send those logs to a log collector. You don\u0026rsquo;t have a lot of budget and you\u0026rsquo;re using a log collector in production but locally you only write out to console (or Debug window in Visual Studio). Maybe you want to persist your logs over a few sessions, maybe you have an automated test environment which hosts your service without console output being saved anywhere. Sure you might implement a file sink for your logs.\nOr you might decide to capture the data from the EventSource your service might be already using.\nEventSource? # This post is for .NET devs. What is an EventSource? It\u0026rsquo;s an old API designed in .NET Framework to pipe events into the ETW (Event Tracing for Windows) system. It has been revamped in .NET Core to be multi-platform and it\u0026rsquo;s the provider of data into dotnet-trace tool. Many internal components emit EventSource events (like GC for example).\nAnd some logging libraries do as well. For example Microsoft.Extensions.Logging is able to export logs to an event source. See AddEventSourceLogger method. It\u0026rsquo;s on by default with the default Host builder.\nReferences:\nEventSource - Getting started EventSource provider PerfView library # So you can capture events using dotnet-trace and view them in standalone PerfView application. It\u0026rsquo;s powerful but not the most comfortable way of dealing with text logs. Not that CSV is much better, but maybe a little.\nPerfView ships either as a GUI app or as a NuGet package Microsoft.Diagnostics.Tracing.TraceEvent. You can use the library to either:\nopen a file produced by dotnet-trace - using new EventPipeEventSource(inputFileName). listen to any events from a provider - using new TraceEventSession(sessionName) with session.EnableProvider(providerName). I opted for the second one, because it allows us to start capturing events as soon as the application starts and it can simultaneously process events from multiple processes.\nFor example to listen to events from the Microsoft-Extensions-Logging EventSource ( source):\nusing (TraceEventSession session = new TraceEventSession(\u0026#34;MyTraceSession\u0026#34;)) { // Enable the ETW session to listen for events from an EventSource session.EnableProvider(\u0026#34;Microsoft-Extensions-Logging\u0026#34;); // filter events to only a specific type and process with callback // we might only listen for MessageJson events for example session.Source.Dynamic.AddCallbackForProviderEvents( (string providerName, string eventName) =\u0026gt; providerName == \u0026#34;Microsoft-Extensions-Logging\u0026#34; \u0026amp;\u0026amp; eventName == \u0026#34;MessageJson\u0026#34; ? EventFilterResponse.AcceptEvent : EventFilterResponse.RejectEvent;, (TraceEvent data) =\u0026gt; {/* do stuff with the event */}); session.Source.Process(); // Listen to events and invoke callback for events in the source } The AddCallbackForProviderEvents takes two functions:\nevent filter - which allows you to specify which events your processor supports, event callback - which allows you to process events one by one. So for me the callback read data from TraceEvent object and emitted them to CSV files. The event object contains info about which process emitted it and the payload with data.\nYou can either process event with typed handlers (which I didn\u0026rsquo;t manage to do because you need to generate a C# class from ETW manifest) or a dynamic handler. In this case I\u0026rsquo;m reading dynamic properties from payload like this:\nTimeStamp = data.TimeStamp.ToString(\u0026#34;s\u0026#34;), ThreadId = data.ThreadID.ToString(\u0026#34;X\u0026#34;), TagId = new EventId((int)data.PayloadByName(\u0026#34;EventId\u0026#34;)).ToTagId(), Level = ((LogLevel)data.PayloadByName(\u0026#34;Level\u0026#34;)).ToString(), LoggerName = data.PayloadStringByName(\u0026#34;LoggerName\u0026#34;), Message = data.PayloadStringByName(\u0026#34;FormattedMessage\u0026#34;), ExceptionDetails = data.PayloadStringByName(\u0026#34;ExceptionJson\u0026#34;), If your event source emits structures, you can do this (IDictionary\u0026lt;string, object\u0026gt;)data.PayloadByName(\u0026quot;ComplexColumn\u0026quot;).\nSo I wrote a little bit more code to use the CsvHelper library and write out the CSV files. I\u0026rsquo;m not gonna post all of the code but it goes a little bit like this:\nOn each event:\nGet CSV file name (process name + PID), Create (or get from cache) an instance of the CsvWriter for the file name: if the file didn\u0026rsquo;t exist we create it and write the header, otherwise just append to it Extract a row of data from event, Write a row to the CSV file. References:\nPerfView application tutorials The Microsoft.Diagnostics.Tracing.TraceEvent Library The TraceEvent Library Programmers Guide dotnet-trace tool CsvHelper website Ways to query a CSV file with SQL Building my tool # I decided to build my tool (which was like 4 C# files) into a single standalone executable, because I needed to deploy it into our test environment to collect logs from automated testing.\nAn alternative is to create your tool in a portable way as a dotnet tool but to use it the machine needs to have .NET SDK installed.\nSo here\u0026rsquo;s my configuration to publish a single file:\n\u0026lt;DebugType\u0026gt;Embedded\u0026lt;/DebugType\u0026gt; \u0026lt;SelfContained\u0026gt;true\u0026lt;/SelfContained\u0026gt; \u0026lt;PublishSingleFile\u0026gt;true\u0026lt;/PublishSingleFile\u0026gt; \u0026lt;GenerateDocumentationFile\u0026gt;false\u0026lt;/GenerateDocumentationFile\u0026gt; \u0026lt;EnableCompressionInSingleFile\u0026gt;true\u0026lt;/EnableCompressionInSingleFile\u0026gt; \u0026lt;IncludeAllContentForSelfExtract\u0026gt;true\u0026lt;/IncludeAllContentForSelfExtract\u0026gt; I was able to call dotnet publish and get a nice executable.\nBut I wanted to package it into a NuGet package which would contain just a tools folder with the executable. This took me two days to figure out fully.\nFirst, we want to enabling packing:\n\u0026lt;IsPackable\u0026gt;true\u0026lt;/IsPackable\u0026gt; Next, since we don\u0026rsquo;t want to include build outputs, but instead publish output, we will disable that:\n\u0026lt;!-- don\u0026#39;t include DLL files produced by this project because we\u0026#39;re manually adding publish output --\u0026gt; \u0026lt;IncludeBuildOutput\u0026gt;false\u0026lt;/IncludeBuildOutput\u0026gt; Now, I\u0026rsquo;m going to add two targets - first one triggers the publish action after building and before packing, second one includes the publish output in the package:\n\u0026lt;Target Name=\u0026#34;PublishBeforePack\u0026#34; BeforeTargets=\u0026#34;GenerateNuspec\u0026#34; AfterTargets=\u0026#34;Build\u0026#34;\u0026gt; \u0026lt;CallTarget Targets=\u0026#34;Publish\u0026#34; /\u0026gt; \u0026lt;/Target\u0026gt; \u0026lt;Target Name=\u0026#34;UpdatePackOutput\u0026#34; BeforeTargets=\u0026#34;GenerateNuspec\u0026#34; AfterTargets=\u0026#34;Publish\u0026#34;\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;_PackageFiles Include=\u0026#34;@(PublishItemsOutputGroupOutputs)\u0026#34;\u0026gt; \u0026lt;FinalOutputPath\u0026gt;%(PublishItemsOutputGroupOutputs.OutputPath)\u0026lt;/FinalOutputPath\u0026gt; \u0026lt;PackagePath\u0026gt;tools\u0026lt;/PackagePath\u0026gt; \u0026lt;/_PackageFiles\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Target\u0026gt; Finally, since the package has just a tool, it doesn\u0026rsquo;t need any dependencies. We can read online that property PrivateAssets=\u0026quot;all\u0026quot; removes the dependency from nuspec. But it also removes it from publish. After a while I found some issue on GitHub that mentioned there\u0026rsquo;s another property called Publish and setting it to true preserves the desired behavior for publishing.\n\u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;CsvHelper\u0026#34; Publish=\u0026#34;True\u0026#34; PrivateAssets=\u0026#34;all\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;Microsoft.Diagnostics.Tracing.TraceEvent\u0026#34; Publish=\u0026#34;True\u0026#34; PrivateAssets=\u0026#34;all\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; But now that we\u0026rsquo;ve gotten rid of library dependencies we need to do remove the framework dependency as well:\n\u0026lt;!-- suppress framework dependency when there\u0026#39;s no library dependencies due to PrivateAssets=all --\u0026gt; \u0026lt;SuppressDependenciesWhenPacking\u0026gt;true\u0026lt;/SuppressDependenciesWhenPacking\u0026gt; That\u0026rsquo;s it. Well, there\u0026rsquo;s one thing you need to do if you\u0026rsquo;re still getting errors - disable IncludeSymbols property which would generate the snupkg file.\nWith the package published to a NuGet source, you can use the following to extract the tool to a projects output:\n\u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;LocalLogsExport\u0026#34; Version=\u0026#34;1.0.0\u0026#34; ExcludeAssets=\u0026#34;all\u0026#34; GeneratePathProperty=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;None Include=\u0026#34;$(PkgLocalLogsExport)\\tools\\*.exe\u0026#34; CopyToOutputDirectory=\u0026#34;PreserveNewest\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; References:\nCreate dotnet tool Publish single file MSBuild debugging tool this is a life saver MSBuild targets documentation CallTarget task documentation PackageReference documentation Nuspec documentation I was inspecting the nuspec generate in the obj folder Suppress the \u0026lt;dependencies\u0026gt; element when packing a project #5132 NU5017 reported for symbol packages without clarifying this #10372 that was the issue bugging me a long time Graceful stopping # The way I wrote the tool the CSV file wasn\u0026rsquo;t being flushed on each event. So if you killed it, you\u0026rsquo;d loose some rows only collected in memory. Instead I subscribed to the console interrupt event to try to enable disposal of the CSV writer.\nprivate static void ConfigureProcesTermination(TraceEventSession session) { Console.CancelKeyPress += (_, args) =\u0026gt; { args.Cancel = true; // prevent process from being terminated before cleanup session.Source.StopProcessing(); // allow call to session.Source.Process() to return // and nicely exit the using block }; // if we are being stopped more forcefully let\u0026#39;s try to at least clean up the session from the system AppDomain.CurrentDomain.ProcessExit += (_, _) =\u0026gt; session.Stop(noThrow: true); } And then I was automating execution of my tool with Powershell and had to write this monstrosity to gracefully stop the process:\n# Stop trace capture gracefully (so that any buffers are flushed to disk) by sending it a CTRL+C signal # Apparently there\u0026#39;s no native way to do it in Powershell, so we\u0026#39;re calling a kernel function # see https://stackoverflow.com/a/64930077 # The FreeConsole and AttachConsole isolates the process from the current Powershell process (otherwise the Ctrl+C kills the script itself as well) $MemberDefinition = \u0026#39; [DllImport(\u0026#34;kernel32.dll\u0026#34;)]public static extern bool FreeConsole(); [DllImport(\u0026#34;kernel32.dll\u0026#34;)]public static extern bool AttachConsole(uint p); [DllImport(\u0026#34;kernel32.dll\u0026#34;)]public static extern bool GenerateConsoleCtrlEvent(uint e, uint p); public static void SendCtrlC(uint p) { FreeConsole(); if (AttachConsole(p)) { GenerateConsoleCtrlEvent(0, p); FreeConsole(); } AttachConsole(uint.MaxValue); }\u0026#39; Add-Type -Name \u0026#39;Console\u0026#39; -Namespace \u0026#39;Process\u0026#39; -MemberDefinition $MemberDefinition $(Get-Process -Name \u0026#39;LocalLogsExport\u0026#39;).Id | foreach { [Process.Console]::SendCtrlC($_) } Wait-Process -Name \u0026#39;LocalLogsExport\u0026#39; -Timeout 30 -ErrorAction Ignore "},{"id":1,"href":"/stride/01-nuget-plugins/","title":"1. Using NuGet packages as plugins","section":"Stride","content":" 1. Using NuGet packages as plugins # This post will be describing my journey to get a sensible plugin orchestration and delivery mechanism in place for the Stride game engine. My main goal is to use NuGet architecture as much as possible and deliver a user experience that doesn\u0026rsquo;t require you to be a genius, nor has you copy-pasting things around too much. And because plugins are only meant for build time, and may target something else than your game, they cannot be directly referenced the usual way.\nStride integrates with MSBuild in a few ways. The GameStudio is acting a bit like Visual Studio in that it allows you to add projects to your solution, builds the projects for you when you make code changes, etc. So when thinking about a plugin architecture I immediately thought about using NuGet package references as a way to deliver plugins and resolve their dependencies. Stride already performs NuGet restore and parses the package.assets.json file to browse the dependencies of your projects. What is needed now is a system that allows us to detect plugins and load them into memory of the editor.\nDetecting something is a plugin # Initially I thought it may be enough to have the user mark a PackageReference with StridePlugin property. However, this requires the user to explicitly know what is a plugin and what isn\u0026rsquo;t. And there\u0026rsquo;s an issue with my ease-of-use goal. If the user references a runtime package, say Stride.Physics, we would expect that the plugin package Stride.Physics.Design would be automatically referenced as it is necessary to include it to process physics assets.\nSo I went online and I found PackageType concept which enables us to declare something is a Stride plugin when creating the NuGet package. It\u0026rsquo;s great and it also gives us the ability to filter NuGet.org for packages with the specified type - e.g. https://www.nuget.org/packages?packageType=StridePlugin - which is a huge win for a mechanism of discovering plugins.\nThis is great, however, currently package type is only used by NuGet SDK when installing package first time into your project to detect that for example your not referencing a .NET tool as dependency and it doesn\u0026rsquo;t preserve this information for later. So I have opened an issue on NuGet repo to ask for adding support for it ( #12934). In the meantime I foresee a potential workaround where a target in Stride.Core checks PackageType of your project and if it matches the plugin type it emits a new file into your package for which we can look after your package is unpacked in the nuget cache.\nOk, so given a dependency tree where some deps are plugins we can probably detect the plugins and select them for loading.\nCircular dependencies # So I mentioned earlier how the author of Stride.Physics should be able to declare that plugin package Stride.Physics.Design needs to be included for the user. How do the dependencies look like?\nUser project -\u0026gt; Stride.Physics -?-\u0026gt; Stride.Physics.Design -\u0026gt; Stride.Physics Ok, so in 90% of plugins I\u0026rsquo;m aiming at, there will be a runtime lib and a design lib and the design lib has to reference runtime lib to generate data for the runtime lib to consume.\nNuGet doesn\u0026rsquo;t allow circular dependencies. You won\u0026rsquo;t even create the package (if using regular tooling).\nSo I started thinking. And the idea was good but ultimately doesn\u0026rsquo;t work. But this blog post is about exploring ideas so let\u0026rsquo;s go.\nIndirect reference idea # Let\u0026rsquo;s simplify names - X is runtime lib, X.D is design time lib.\nX.D references X via ProjectReference which will be converted into a package dependency when packed. X.D marks itself as a Stride plugin. X notes \u0026lt;RequiredStridePlugin Include=\u0026quot;Plugin\u0026quot; Version=\u0026quot;1.0.0\u0026quot; /\u0026gt; in its project When X is packed we create a build/X.targets file which will be included in the user project that declares items IncludeStridePlugin matching the requirement. When user project referencing X is built we convert IncludeStridePlugin into PackageReference I learned a bit more about MSBuild while working on this. Especially trying to answer the question how to select the maximum version from the possibly duplicate containing list of IncludeStridePlugin items. And the answer was target batching.\nHere\u0026rsquo;s my code which was placed in Stride.Core targets:\n\u0026lt;!-- First the creator of a plugin will include the following in their runtime project: \u0026lt;ItemGroup\u0026gt; \u0026lt;RequiredStridePlugin Include=\u0026#34;MyPlugin\u0026#34; Version=\u0026#34;1.0.0\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; This target will generate a RequiredStridePlugin.targets file in the buildTransitive folder of the package, so that the consumers of the runtime package will have the following added to their project: \u0026lt;ItemGroup\u0026gt; \u0026lt;IncludeStridePlugin Include=\u0026#34;MyPlugin\u0026#34; Version=\u0026#34;1.0.0\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; --\u0026gt; \u0026lt;Target Name=\u0026#34;_StrideConfigureRequiredPluginsInPackage\u0026#34; BeforeTargets=\u0026#34;Build\u0026#34; Condition=\u0026#34;@(RequiredStridePlugin-\u0026gt;Count()) \u0026amp;gt; 0\u0026#34;\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;_StrideRequiredPlugin Include=\u0026#34;@(RequiredStridePlugin)\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;WriteLinesToFile File=\u0026#34;$(IntermediateOutputPath)$(PackageId).targets\u0026#34; Lines=\u0026#34;\u0026amp;lt;Project\u0026amp;gt;;@(_StrideRequiredPlugin-\u0026gt;\u0026#39;\u0026amp;lt;ItemGroup\u0026amp;gt;\u0026amp;lt;IncludeStridePlugin Include=\u0026amp;quot;%(Identity)\u0026amp;quot; Version=\u0026amp;quot;%(Version)\u0026amp;quot; /\u0026amp;gt;\u0026amp;lt;/ItemGroup\u0026amp;gt;\u0026#39;);\u0026amp;lt;/Project\u0026amp;gt;\u0026#34; Overwrite=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;!-- TODO: how can we allow the user to add their own targets? Using PackageId as name of the file is required for it to be loaded. see https://github.com/NuGet/docs.microsoft.com-nuget/blob/main/docs/reference/errors-and-warnings/NU5129.md --\u0026gt; \u0026lt;None Include=\u0026#34;$(IntermediateOutputPath)$(PackageId).targets\u0026#34; PackagePath=\u0026#34;build\\\u0026#34; Pack=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;None Include=\u0026#34;$(IntermediateOutputPath)$(PackageId).targets\u0026#34; PackagePath=\u0026#34;buildTransitive\\\u0026#34; Pack=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Target\u0026gt; \u0026lt;!-- Based on IncludeStridePlugin declared we will try to load the latest version of the plugin. This only matters if two referenced library require the same plugin but with different versions. \u0026lt;IncludeStridePlugin Include=\u0026#34;X\u0026#34; Version=\u0026#34;1.2.0\u0026#34; /\u0026gt; \u0026lt;IncludeStridePlugin Include=\u0026#34;X\u0026#34; Version=\u0026#34;1.4.0\u0026#34; /\u0026gt; becomes \u0026lt;_StridePluginReference Include=\u0026#34;X\u0026#34; Version=\u0026#34;1.4.0\u0026#34; /\u0026gt; --\u0026gt; \u0026lt;!-- We use the Inputs and Outputs to allow Target batching and group IncludeStridePlugin by package name --\u0026gt; \u0026lt;Target Name=\u0026#34;_StrideIncludePluginReferencesByHighestVersion\u0026#34; BeforeTargets=\u0026#34;CollectPackageReferences\u0026#34; Inputs=\u0026#34;@(IncludeStridePlugin)\u0026#34; Outputs=\u0026#34;__%(Identity)\u0026#34;\u0026gt; \u0026lt;!-- for each Version update _TempStridePluginVersion if Version \u0026gt; _TempStridePluginVersion --\u0026gt; \u0026lt;CreateProperty Value=\u0026#34;%(IncludeStridePlugin.Version)\u0026#34; Condition=\u0026#34;\u0026#39;$(_TempStridePluginVersion)\u0026#39; == \u0026#39;\u0026#39; OR $([System.Version]::Parse(%(IncludeStridePlugin.Version))) \u0026amp;gt; $([System.Version]::Parse($(_TempStridePluginVersion)))\u0026#34;\u0026gt; \u0026lt;Output TaskParameter=\u0026#34;Value\u0026#34; PropertyName=\u0026#34;_TempStridePluginVersion\u0026#34; /\u0026gt; \u0026lt;/CreateProperty\u0026gt; \u0026lt;!-- include the plugin reference with the greatest version --\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;_StridePluginReference Include=\u0026#34;%(IncludeStridePlugin.Identity)\u0026#34; Version=\u0026#34;$(_TempStridePluginVersion)\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;!-- unset property for the next target invocation --\u0026gt; \u0026lt;CreateProperty Value=\u0026#34;\u0026#34;\u0026gt; \u0026lt;Output TaskParameter=\u0026#34;Value\u0026#34; PropertyName=\u0026#34;_TempStridePluginVersion\u0026#34; /\u0026gt; \u0026lt;/CreateProperty\u0026gt; \u0026lt;/Target\u0026gt; \u0026lt;!-- For each _StridePluginReference we will add a PackageReference if not already set by the user (allows user to override the version). Needs to be invoked before CollectPackageReferences target for the packages to be picked up for build correctly. We only include build assets from the package to avoid the plugin being used for compilation. --\u0026gt; \u0026lt;Target Name=\u0026#34;_StrideIncludePluginReferencesFromPackages\u0026#34; AfterTargets=\u0026#34;_StrideIncludePluginReferencesByHighestVersion\u0026#34; BeforeTargets=\u0026#34;CollectPackageReferences\u0026#34; Condition=\u0026#34;@(_StridePluginReference-\u0026gt;Count()) \u0026amp;gt; 0\u0026#34;\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;@(_StridePluginReference)\u0026#34; Exclude=\u0026#34;@(PackageReference)\u0026#34; IncludeAssets=\u0026#34;build;buildTransitive\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Target\u0026gt; So why doesn\u0026rsquo;t it work?\nWell, the issue is simple - you can\u0026rsquo;t run targets from other packages during Restore because they\u0026rsquo;re not restored yet. And if you cannot add the PackageReference before the restore then it\u0026rsquo;s not restored, it\u0026rsquo;s not in the dependency graph, not populated in project.assets.json and not taken into account during build.\nMoreover, I found this\nThere are a few things that must not be done in packages\u0026rsquo; .props and .targets, such as not specifying properties and items that affect restore, as those will be automatically excluded.\nSome examples of items that must not be added or updated: PackageReference, PackageVersion, PackageDownload, etc. So it looks like we probably can\u0026rsquo;t get away with indirectly including package references just yet.\nMaking plugins not have dependencies # Something I haven\u0026rsquo;t considered too much but crossed my mind - what if we enforce PrivateAssets=all on any dependency of the plugin, thus making it not have dependencies at all, thus none of them will be circular? Will need to think more.\nNext? # This post will be updated over time. For the main issue spawning these ideas look at Stride Plugin RFC.\n"},{"id":2,"href":"/iqa-management-hub/01-writing-system-tests/","title":"1. Writing system tests","section":"IQA Management Hub","content":" 1. Writing system tests # So here I am - I made a decision to make a rewrite of the service. Where do I start? Well, the number one thing I want to do is to try to follow good practices. I don\u0026rsquo;t want to just write this thing from scratch and swap them out in one go, possibly breaking tens of existing users with something I haven\u0026rsquo;t tested. So I need to first document how the existing service works. I will use system tests for that.\nWhat are system tests? # If you heard about the test pyramid you may be familiar with service or integration tests. The way I define system tests is an external testing process making calls to a locally deployed application and validating the general behavior.\nWhat this means is that the system test mostly doesn\u0026rsquo;t care about implementation details and focuses on externalities. For a web API that means most tests should be performable by making HTTP requests against the service. In my case because I need to document a bit deeper how the service behaves I will also be connecting to the database to inspect it, clean up state created with the tests, etc.\nSetup # I\u0026rsquo;ve created a new .NET 6 project with the XUnit template and added Xunit.DependencyInjection which allows me to use dependency injection, configuration files and advanced logging controls.\nI\u0026rsquo;ve also created a project for models and generated classes using Entity Framework Core dotnet tool. The app uses postgres so I installed Npgsql.EntityFrameworkCore.PostgreSQL. Then I ran dotnet ef dbcontext scaffold to set up the models. I like to keep models mostly pure, so all my EF configuration ends up in the DbContext class.\nFinally I needed some tools for making the HTTP requests and I opted for writing my own, specifically tailored to this app.\nHttpClient # I have read a lot in the past on how to use the HttpClient. The most recent advice says to use IHttpClientFactory from the Microsoft.Extensions.Http package. It allows us to use dependency injection to configure the client. And you can name the client to have multiple configurations.\nBut the main benefit is that the factory intelligently caches the HttpClientHandler instances which hold system resources. A big issue of manual handler management is port exhaustion when you create to many instances. It\u0026rsquo;s a general issue and I\u0026rsquo;ve seen systems die from this.\nIn .NET Core a new system of layers has been designed on top of the HttpClientHandler. There\u0026rsquo;s many small and reusable instances of DelegatingHandler which act very similarly to ASP.NET middleware. It can be used for injecting headers or otherwise modifying the request or the response.\nCookies # For accessing the management hub website you login with an email and a password and get a session cookie. Standard stuff. Took me 2 days to make it work.\nSo the first thing is cookies. As mentioned above, the HttpClientHandler is reused by multiple HttpClient instances to save on system resources. This means that the built in cookie management mechanism are out the window, because they\u0026rsquo;d be shared across multiple clients. We need the cookie store to be specific to a given test. Also we need to allow tests to run in parallel for fast CI times later on.\nSo I looked around and came up with this middleware (shout out to damianh\u0026rsquo;s gist which saved me from parsing cookies by hand)\n/// \u0026lt;summary\u0026gt; /// Message handler which substitutes the handling of cookies of the \u0026lt;see cref=\u0026#34;HttpClientHandler\u0026#34;/\u0026gt;. /// It allows to reuse a single instance of a underlying handler across multiple cookie sessions. /// \u0026lt;/summary\u0026gt; public class CookieSessionMessageHandler : DelegatingHandler { public static readonly HttpRequestOptionsKey\u0026lt;CookieContainer\u0026gt; CookieContainerOption = new(\u0026#34;CookieContainer\u0026#34;); protected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken) { if (!request.Options.TryGetValue(CookieContainerOption, out var cookieContainer)) return await base.SendAsync(request, cancellationToken); if (cookieContainer.Count \u0026gt; 0) request.Headers.Add(\u0026#34;Cookie\u0026#34;, cookieContainer.GetCookieHeader(request.RequestUri!)); var response = await base.SendAsync(request, cancellationToken); if (response.Headers.TryGetValues(\u0026#34;Set-Cookie\u0026#34;, out var cookieValues)) foreach (var cookieValue in cookieValues) { cookieContainer.SetCookies(request.RequestUri!, cookieValue); } return response; } } I\u0026rsquo;ve cleaned up the code a bit for this post to keep it shorter, but I also have some debug logs in there.\nWe can see I\u0026rsquo;m using the HttpRequestOptions collection to store the CookieContainer. This means I needed to write custom extensions over HttpClient for things like GetStringAsync(), because I wanted to build my own HttpRequestMessage and add the cookie container.\nThe middleware also works if we don\u0026rsquo;t care about cookies - this is generally a good practice - make your middleware opt-in rather than opt-out.\nFor sending the cookies we\u0026rsquo;re using a method of the CookieContainer called GetCookieHeader(url). For saving new cookies we\u0026rsquo;re using the SetCookies(url, cookieHeader) method. If you happen to have issues with it, checkout the helper class SetCookieHeaderValue, from Microsoft.Net.Http.Headers package, to parse the headers.\nSo with this cookie middleware I was able to log in, but afterwards something wasn\u0026rsquo;t working and I had to debug it.\nRedirects # The session cookie system of Rails works in a way that sends a new cookie with each request. I believe it\u0026rsquo;s meant to prevent an attacker from stealing a session in some way (see Rails guide on security).\nWith this new cookie middleware I was sending redirect requests with the same cookie as the original request.\nsequenceDiagram CookieMiddleware-\u003e\u003eHttpClientHandler: Headers[\"Cookie\"] HttpClientHandler-\u003e\u003eWebsite: HTTP request Website--\u003e\u003eHttpClientHandler: 302 Headers[\"Location\"] HttpClientHandler-\u003e\u003eWebsite: HTTP GET request to new location Website--\u003e\u003eHttpClientHandler: 200 OK HttpClientHandler--\u003e\u003eCookieMiddleware: Headers[\"Set-Cookie\"] And this wasn\u0026rsquo;t working, so I had to write my own redirect layer on top of it.\n/// \u0026lt;summary\u0026gt; /// Message handler which substitutes the handling of redirects of the \u0026lt;see cref=\u0026#34;HttpClientHandler\u0026#34;/\u0026gt;. /// We needed redirection layer on top of the custom cookie middleware. /// \u0026lt;/summary\u0026gt; public class FollowRedirectsMessageHandler : DelegatingHandler { protected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync( HttpRequestMessage request, CancellationToken cancellationToken) { var response = await base.SendAsync(request, cancellationToken); if (response.Headers.TryGetValues(\u0026#34;Location\u0026#34;, out var locations)) { var redirectRequest = new HttpRequestMessage(HttpMethod.Get, locations.First()); if (request.Options is IDictionary\u0026lt;string, object?\u0026gt; requestOptions \u0026amp;\u0026amp; redirectRequest.Options is IDictionary\u0026lt;string, object?\u0026gt; newOptions) foreach (var kvp in requestOptions) { newOptions.Add(kvp.Key, kvp.Value); } return await this.SendAsync(redirectRequest, cancellationToken); } return response; } } Basically whenever there\u0026rsquo;s a Location header present we make a new request there and copy all the request options. Note how I used base.SendAsync and this.SendAsync to go directly down the request pipeline or call this method recursively to allow for following multiple redirects.\nsequenceDiagram RequestMiddleware-\u003e\u003eCookieMiddleware: . CookieMiddleware-\u003e\u003eHttpClientHandler: Headers[\"Cookie\"] HttpClientHandler-\u003e\u003eWebsite: HTTP request Website--\u003e\u003eHttpClientHandler: 302 Headers[\"Location\"] HttpClientHandler--\u003e\u003eCookieMiddleware: Headers[\"Set-Cookie\"] CookieMiddleware--\u003e\u003eRequestMiddleware: . RequestMiddleware-\u003e\u003eCookieMiddleware: GET redirected CookieMiddleware-\u003e\u003eHttpClientHandler: Headers[\"Cookie\"] HttpClientHandler-\u003e\u003eWebsite: HTTP request Configuring the HttpClient # Finally we go to the Configure method to set up dependency injection.\nservices.AddTransient\u0026lt;FollowRedirectsMessageHandler\u0026gt;(); services.AddTransient\u0026lt;CookieSessionMessageHandler\u0026gt;(); services.AddHttpClient(RequestBuilder.HttpClientName) .ConfigurePrimaryHttpMessageHandler(() =\u0026gt; new HttpClientHandler { AllowAutoRedirect = false, UseCookies = false, }) // Ordering of the handlers is important, from top (outermost) to bottom .AddHttpMessageHandler\u0026lt;FollowRedirectsMessageHandler\u0026gt;() .AddHttpMessageHandler\u0026lt;CookieSessionMessageHandler\u0026gt;(); We disable redirects and cookies on the HttpClientHandler because we add middleware to handle it instead.\nEmails # Nowadays it feels like emails are a bit of a thing of the past, but at the same time they are everywhere. Products like SendGrid enable you to make a HTTP request to their server and they take care of sending the email. But without modern solutions it is still possible to send email over the basic SMTP protocol. And that is exactly what Management Hub is doing.\nUsually you could write the email to memory or the file system for testing. But to have additional confidence in my system I want to make sure the mailing library I use is able to send the email properly over SMTP.\nSo in my tests I included the library netDumbster - a simple SMTP server, which allows me to receive emails sent by the web service. In order to not have to run my tests with admin privileges I configured both the service and the library to work on port 4025.\nI\u0026rsquo;ve used the IHostedService interface on my wrapper around the SMTP server to start it once for the whole testing process.\nservices.AddSingleton\u0026lt;EmailProvider\u0026gt;(); // auto-start email server services.AddSingleton\u0026lt;IHostedService\u0026gt;(sp =\u0026gt; sp.GetRequiredService\u0026lt;EmailProvider\u0026gt;()); Then the test receives the wrapper from the DI framework and checks the incoming messages (in a polling fashion in case there\u0026rsquo;s a delay between HTTP response and email being sent) for the one matching a predicate.\npublic async Task\u0026lt;SmtpMessage\u0026gt; PollAsync(Func\u0026lt;SmtpMessage, bool\u0026gt; predicate, TimeSpan? timeout = null) { timeout ??= TimeSpan.FromSeconds(10); using var cts = new CancellationTokenSource(timeout.Value); while (true) { Assert.False(cts.Token.IsCancellationRequested, \u0026#34;Smtp polling timeout\u0026#34;); var message = this.emailMessages.SingleOrDefault(predicate); if (message != null) return message; await Task.Delay(PollingInterval); } } Unique persistent users # As I started writing the tests I decided it\u0026rsquo;s a good idea to have each test go through a full cycle of setup, assertion and cleanup. This means creating a new user, setting up new objects in the database, executing some actions, and finally removing all created things from the database. I found it an issue to give all tests the same email address, because they can run in parallel and I wouldn\u0026rsquo;t want them to mess with one another. But random ids are also bad, because they can lead a test to fail some of the time if a bad id is causing an issue. Or in my case, if a test fails and there\u0026rsquo;s a bug in the cleanup code, having a persistent id can help execute the cleanup at the beginning of the next test run.\nI used a simple mechanism of having a helper method which takes a string argument with [CallerMemberName] which is the name of the test method, an integer seed (in case one test needs multiple users) and returns a user context object, which implements IDisposable to perform cleanup.\nFrom the name of the method I calculate a stable hash (don\u0026rsquo;t use GetHashCode - it is stable only within a single run) using the FarmHash64 algorithm from FastHashes and converting the result to a Base64 string (which makes a valid email username). Also needed to cast the email ToLower() because the service was configured with case insensitive email comparisons.\nprivate static string GenerateEmailFromString(string input, int seed = 0) { var inputAsByteSpan = MemoryMarshal.Cast\u0026lt;char, byte\u0026gt;(input.AsSpan()); // We\u0026#39;re using a stable hashing algorithm here to ensure // the same email is use for a given test between runs string hash = Convert.ToBase64String(HashAlgorithm.ComputeHash(inputAsByteSpan)); string seedStr = seed != 0 ? seed.ToString() : string.Empty; // Need to call ToLower, because Devise is doing that before saving the email to the database return $\u0026#34;user_{hash.ToLowerInvariant()}{seedStr}@example.com\u0026#34;; } "},{"id":3,"href":"/iqa-management-hub/","title":"IQA Management Hub","section":"Marian's Blog","content":" IQA Management Hub # You may not have heard about it, but there\u0026rsquo;s a sport called quadball, formerly known as quidditch. I discovered it when I was in highschool, got really into it. But obviously sport is not just the players. It\u0026rsquo;s referees, team managers, league organizers. So at some point I decided to volunteer for the IQA - International Quadball Association. And my favorite capacity of volunteering is as a software engineer.\nIQA doesn\u0026rsquo;t have a lot needs for a big team of programmers. They run a website with news blog posts and a service called Management Hub. It\u0026rsquo;s a single place for admins of National Governing Bodies (country members of IQA) to provide official statistics to the IQA and a referee hub for certifying referees.\nSo at the moment of writing I\u0026rsquo;m the one maintaining it.\nAnd as I set off in my role I decided I don\u0026rsquo;t want to do any rewrites, just keep the existing codebase alive. Easier said than done. It was written in ruby on rails and it\u0026rsquo;s cool. BUT I can\u0026rsquo;t spend the time to learn ruby in depth to migrate to a newer version where a lot of things will break. AND I like C# a lot. So I\u0026rsquo;m going to try to rewrite this site into ASP.NET Core and try to fix a lot of issues that piled up over time.\n"},{"id":4,"href":"/stride/","title":"Stride","section":"Marian's Blog","content":" Stride # I\u0026rsquo;m a contributor to the Stride game engine. It\u0026rsquo;s open source, it\u0026rsquo;s C# - I love it.\nStride is a huge code base with over 100 projects and many users all over the world with a variety of needs. My main area of contribution looks at core libraries, serialization system, asset pipeline and also the plugin architecture.\n"},{"id":5,"href":"/telemetry/","title":"Telemetry","section":"Marian's Blog","content":" Telemetry # I\u0026rsquo;m a big fan of observability of services, being able to say how they are performing and what are the issues by looking at dashboards and querying logs. In order to get that nice picture one needs to instrument their applications to emit telemetry signals - logs, metrics, traces.\n"}]